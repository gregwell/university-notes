# Parallel Programming Reports

This file contains the merged class reports **with the code attached**. (in Polish ğŸ˜)

# **Sprawozdanie z Laboratorium 1.**

**Cel:**
- przeprowadzenie pomiaru czasu CPU i zegarowego wykonania operacji
- organizacja Å›rodowiska tworzenia oprogramowania w systemie Linux (make, cc itp.)

**Wykonanie nastÄ™pujÄ…cych Ä‡wiczeÅ„:**

1. Skopiowanie wskazanych plikÃ³w tj. Makefile, pomiar_czasu.c, pomiar_czasu.h, moj_program.c) do katalogu roboczego lab_1.
2. Modyfikacja pliku ÅºrÃ³dÅ‚owego *moj_program.c* umieszczajÄ…c wywoÅ‚ania prodecur pomiaru i wydruku czasu. (plik w zaÅ‚Ä…czniku nr 1.)
3. Modyfikacja pliku *Makefile* tak aby poprawnie tworzyÄ‡ plik binarny moj_program poprzez:
- dodanie plikÃ³w *moj_program.c, pomiar_czasu.h* w linii zaleÅ¼noÅ›ci pliku *moj_program.o*
- wpisanie polecenia kompilacji z odpowiedniemi opcjami pliku ÅºrÃ³dÅ‚owego moj_program.c (plik stanowi zaÅ‚Ä…cznik 2.)

4. Uruchomienie kodu.

5. Analiza czasÃ³w wykonania przedstawia siÄ™ nastÄ™pujÄ…co:

- czas wykonania 100000 operacji arytmetycznych mnoÅ¼enia:

    czas CPU: 0.005704, czas zegarowy: 0.017513

- czas wykonania 100000 operacji wejscia/wyjscia:

    czas CPU: 0.005536, czas zegarowy: 0.061648

W ramach zadaÅ„ dodatkowych zrealizowano:

1. Przeniesienie plikÃ³w zwiÄ…zanych z pomiarem czasu do odrÄ™bnego katalogu pomiar_czasu.
2. W katalogu pomiar_czasu utworzono bibliotekÄ™ libpomiar_czasu.a z pliku pomiar_czasu.o poleceniem ar -rs libpomiar_czasu.a pomiar_czasu.o
3. Zmodyfikowano plik Makefile tak aby zamiast plikow ÅºrÃ³dÅ‚owych i poÅ›rednich pomiaru czasu korzystaÅ‚ z biblioteki:
- usunÄ…Å‚em odniesienia do plikÃ³w pomiar_czasu.c i pomiar_czasu.o
- Nadano wÅ‚asciwe wartoÅ›ci symbolom LIB i INC.
- Dodano wÅ‚aÅ›ciwe wykorzsytanie symobli LIB i INC w trakcie kompilacji.

**Wnioski:**

1. Wykorzystanie makefile uÅ‚atwia pracÄ™ i tworzenie kodu binarnego z plikÃ³w ÅºrÃ³dÅ‚owych.
2. Pomiar czasu wykazaÅ‚, Å¼e operacje wejÅ›Ä‡ia/wyjÅ›cia zajmujÄ… wiÄ™cej czasu zegarowego aniÅ¼eli operacje arytmetyczne. 
3. Pomiar czasu wykazaÅ‚, Å¼e czas wykonania przez CPU jest podobny w obu typach operacji, jednak czekaÄ‡ musimy dÅ‚uÅ¼ej na wykonanie operacji przez twardy dysk, stÄ…d taka rÃ³Å¼nica w czasie.
4. W moim przypadku wersja zoptymalizowana nie wykazaÅ‚a wielkich zmian w stosunku do wersji do debugowania.

ZaÅ‚Ä…cznik 1. Plik moj_program.c

```jsx
#include<stdlib.h>
#include<stdio.h>
#include<time.h>
#include "pomiar_czasu.h"
 
const int liczba = 100000;

main(){

  double a, b, c;
  int i,j,k,l;
  double t1;
  double t2;

  k = -50000;

  inicjuj_czas();
  for(i=0;i<liczba;i++){

    printf("%d ",k+i);

  }
  drukuj_czas();
	
  printf("\n");

  printf("Czas wykonania %d operacji wejscia/wyjscia: \n",liczba);

  a = 1.000001;

  t1=czas_zegara();
  t2=czas_CPU();
  for(i=0;i<liczba;i++){

    a = 1.000001*a+0.000001; 

  }
  t1=czas_zegara()-t1;
  t2=czas_CPU()-t2;

  printf("Wynik operacji arytmetycznych: %lf\n", a);
  printf("Czas wykonania %d operacji arytmetycznych: \n",liczba);

  printf("Czas zegara t1= %lf \n", t1);
  printf("czas cpu t2 = %lf \n", t2);

}
```

ZaÅ‚Ä…cznik 2. Plik Makefile

```jsx
# kompilator c
CCOMP = gcc

# konsolidator
LOADER = gcc

# opcje optymalizacji:
# wersja do debugowania
# OPT = -g -DDEBUG
# wersja zoptymalizowana do mierzenia czasu
OPT = -O3

# pliki naglowkowe
INC = -I../pomiar_czasu

# biblioteki
LIB = -L../pomiar_czasu -lm

# zaleznosci i komendy
moj_program: moj_program.o pomiar_czasu.o
	$(LOADER) $(OPT) moj_program.o pomiar_czasu.o -o moj_program $(LIB)

# jak uzyskac plik moj_program.o ?
moj_program.o: moj_program.c 
	$(CCOMP) -c $(OPT) moj_program.c $(INC)
	
pomiar_czasu.o: pomiar_czasu.c pomiar_czasu.h
	$(CCOMP) -c $(OPT) pomiar_czasu.c

clean:
	rm -f *.o
```

# **Sprawozdanie z laboratorium 5.**

6.11.2020, godz. 8:00

### Cel:

Celem laboratorium byÅ‚o nabycie umiejÄ™tnoÅ›ci tworzenia i implementacji programÃ³w rÃ³wnolegÅ‚ych w Å›rodowisku **Pthreads**.

### Wykonanie:

W ramach zajÄ™Ä‡ zrealizowaÅ‚em nastÄ™pujÄ…ce kroki:

- UtworzyÅ‚em katalog roboczy lab_5
- Skopiowalem ze strony przedmiotu paczkÄ™ ***pthreads_suma.tgz,*** ktÃ³rÄ… rozpakowalem nastÄ™pnie w podkatalogu zad_1
- SkopiwaÅ‚em ze strony przedmiotu paczkÄ™ ***libpomiar_czasu.tgz,*** ktÃ³rÄ… uÅ¼yÅ‚em jako biblioteki pomiaru czasu, portrzebnej do dziaÅ‚ania programu.
- UruchomiÅ‚em i przetestowaÅ‚em dziaÅ‚anie dla rÃ³Å¼nej liczby wÄ…tkÃ³w

![images/Untitled.png](images/Untitled.png)

Uruchomienie 1.

![images/Untitled%201.png](images/Untitled%201.png)

Uruchomienie 2.

**Jak wyglÄ…da prosty wzorzec zrÃ³wnoleglenia pÄ™tli?**

Gdy rozpatrujemy zrÃ³wnoleglanie, ze wzglÄ™du na skalÄ™ w jakiej siÄ™ ono odbywa moÅ¼emy rozrÃ³Å¼niÄ‡ obliczenia rÃ³wnolegÅ‚e na poziomie bitÃ³w, instrukcji, **danych** oraz zadaÅ„.

**ZrÃ³wnoleglanie na poziomie przetwarzania danych** 

- jest wÅ‚aÅ›ciwe przetwarzaniu iteracyjnemu i skupia siÄ™ na rozdzielaniu danych pomiÄ™dzy rÃ³Å¼ne jednostki obliczeniowe w taki sposÃ³b aby zminimalizowaÄ‡ ich wzajemne zaleÅ¼noÅ›ci
- Rozdzielenie danych nie zawsze jest moÅ¼liwe. JeÅ›li na przykÅ‚ad obliczenia w kaÅ¼dej kolejnej iteracji zaleÅ¼Ä… od wynikÃ³w poprzedniej, to zrÃ³wnoleglenie pÄ™tli nie jest moÅ¼liwe.
- kaÅ¼dy wÄ…tek/proces
    - otrzymuje pewnÄ… liczbÄ™ iteracji pÄ™tli do wykonania
    - posiada wÅ‚asny indeks iteracji
    - wykonanie jest najczÄ™Å›ciej synchroniczne

W naszym przykÅ‚adzie **kaÅ¼dy wÄ…tek** otrzymuje fragment caÅ‚ki do policzenia, co przeskok rÃ³wny liczbie wÄ…tkÃ³w.

```objectivec
int i; 
double calka = 0.0; 
for(i=my_start; i<my_end; i+=my_stride){ 
double x1 = a + i*dx; 
calka += 0.5*dx*(funkcja(x1)+funkcja(x1+dx));
} 
pthread_mutex_lock(&muteks);
calka_global += calka; 
pthread_mutex_unlock(&muteks); 
pthread_exit((void*)0)
```

- W zadaniu obliczania sumy tablicy caÅ‚Ä… tablicÄ™ przesyÅ‚amy z pamiÄ™ci do rdzeni procesora
- Mimo zwiÄ™kszania liczby wÄ…tkÃ³w czasy w pewnym momencie przestajÄ… siÄ™ zmniejszaÄ‡ - dzieje siÄ™ to ze wzglÄ™du na ograniczonÄ… przepustowoÅ›Ä‡ magistrali
- Czas obliczeÅ„ w wersji rozdzielonej na wÄ…tki i w wersji synchronicznej jest bardzo podobny.
- WyÅ›cig przy sumowaniu ostatecznego wyniku moÅ¼na wyeliminowaÄ‡ stosujÄ…c muteksy oraz sekcjÄ™ krytycznÄ… w ktÃ³rej sumujemy wynik.

### CzÄ™Å›Ä‡ dalsza

- Pobrano pliku ***pthreads_calka.c*** i ***Makefile***
- dodano do programu wariant obliczania caÅ‚ki w sposÃ³b wielowÄ…tkowy przy uÅ¼yciu wzorca zrÃ³wnoleglenia pÄ™tli
- zastosowano cykliczny wariant dekompozycji pÄ™tli
- program gÅ‚Ã³wny wywoÅ‚uje funkcje caÅ‚ka_zrownoleglenie_petli, ktÃ³ra zwraca wartoÅ›Ä‡ obliczonej caÅ‚ki
- dodano do programu wariant obliczania caÅ‚ki w sposÃ³b wielowÄ…tkowy przy uÅ¼yciu wzorca dekompozycji w dziedzinie problemu
- w utworzonej wersji dekompozycji wÄ…tku tworzona jest tablica o typie struktury, w ktÃ³rej przechowujemy dane potrzebne do caÅ‚kowania dla poszczegÃ³lnego wÄ…tku.
- kaÅ¼dy wÄ…tek dostaje rÃ³wny obszar dziaÅ‚ania
- obliczane jest N i poprawiane dx.
- Tworzymy wÄ…tki , bez synchronizacji. Nie uÅ¼ywamy muteksÃ³w.
- Wynik w tej wersji rÃ³Å¼ni siÄ™ od wyniku z wersji sekwencyjnej poniewaÅ¼ wartoÅ›Ä‡ dx dla wersji sekwencyjnej obliczamy z caÅ‚ego przedziaÅ‚u caÅ‚kowania a w tej wersji dx liczymy osobno dla kaÅ¼dego obszaru dziaÅ‚ania tego wÄ…tku.

![images/Untitled%202.png](images/Untitled%202.png)

![images/Untitled%203.png](images/Untitled%203.png)

![images/Untitled%204.png](images/Untitled%204.png)

pthreads_calka.c

```objectivec
#include<stdlib.h>
#include<stdio.h>
#include<pthread.h>
#include<math.h>
#include"pomiar_czasu.h"
double funkcja ( double x );
double funkcja ( double x ){ return( sin(x) ); }
double calka_sekw(double a, double b, double dx, int N);
double calka_zrownoleglenie_petli(double a, double b, double dx, int N);
double calka_dekompozycja_obszaru(double a, double b, double dx, int N);
pthread_mutex_t muteks;
#define LICZBA_W_MAX 1000
int l_w_global=0;
struct dane{
 double a, b, dx, wynik;
 int N, id;
};
int main( int argc, char *argv[] ){
 int i; 
 double t1,t2,t3;
 double a, b, dx, calka;
 
 printf("\nProgram obliczania caÅ‚ki z funkcji (sinus) metodÄ… trapezÃ³w.\n");
 a = 0.0;
 //printf("\nPodaj lewy kraniec przedziaÅ‚u caÅ‚kowania: "); scanf("%lf", &a);
 b = M_PI;
 //printf("\nPodaj prawy kraniec przedziaÅ‚u caÅ‚kowania: "); scanf("%lf", &b);
 printf("\nPodaj wysokoÅ›Ä‡ pojedynczego trapezu: "); scanf("%lf", &dx);
 int N = ceil((b-a)/dx);
 double dx_adjust = (b-a)/N;
 printf("Obliczona liczba trapezÃ³w: N = %d\n", N);
 printf("\nPodaj liczbÄ™ wÄ…tkÃ³w: "); scanf("%d", &l_w_global);
 printf("\nPoczatek obliczeÅ„ sekwencyjnych\n");
 printf("a %lf, b %lf, n %d, dx %.12lf (dx_adjust %.12lf)\n", a, b, N, dx, dx_adjust);
 t1 = czas_zegara();
 calka = calka_sekw(a, b, dx_adjust, N);
 t1 = czas_zegara() - t1;
 printf("\nKoniec obliczen sekwencyjnych\n");
 printf("\tCzas wykonania %lf. \tObliczona caÅ‚ka = %.15lf\n", t1, calka);
 printf("\nPoczatek obliczeÅ„ rÃ³wnolegÅ‚ych (zrÃ³wnoleglenie pÄ™tli)\n");
 printf("a %lf, b %lf, n %d, dx %.12lf (dx_adjust %.12lf)\n", a, b, N, dx, dx_adjust);
 t1 = czas_zegara();
 calka = calka_zrownoleglenie_petli(a, b, dx_adjust, N);
 pthread_mutex_init(&muteks,NULL);
 t1 = czas_zegara() - t1;
 printf("\nKoniec obliczen rÃ³wnolegÅ‚ych (zrÃ³wnoleglenie pÄ™tli) \n");
 printf("\tCzas wykonania %lf. \tObliczona caÅ‚ka = %.15lf\n", t1, calka);
 printf("\nPoczatek obliczeÅ„ rÃ³wnolegÅ‚ych (dekompozycja obszaru)\n");
 printf("a %lf, b %lf, n %d, dx %.12lf (dx_adjust %.12lf)\n", a, b, N, dx, dx_adjust);
 t1 = czas_zegara();
 calka = calka_dekompozycja_obszaru(a, b, dx_adjust, N);
 t1 = czas_zegara() - t1;
 printf("\nKoniec obliczen rÃ³wnolegÅ‚ych (dekompozycja obszaru) \n");
 printf("\tCzas wykonania %lf. \tObliczona caÅ‚ka = %.15lf\n", t1, calka);
}
double calka_sekw(double a, double b, double dx, int N){
 int i;
 double calka = 0.0;
 for(i=0; i<N; i++){
 double x1 = a + i*dx;
 calka += 0.5*dx*(funkcja(x1)+funkcja(x1+dx));
 //printf("i %d, x1 %lf, funkcja(x1) %lf, caÅ‚ka = %.15lf\n",
 // i, x1, funkcja(x1), calka);
 }
 return(calka);
}
static double calka_global=0.0;
static double a_global;
static double b_global;
static double dx_global;
static int N_global;
void* calka_fragment_petli_w(void* arg_wsk);
double calka_zrownoleglenie_petli(double a, double b, double dx, int N){
 //printf("a %lf, b %lf, dx %lf, n %d\n", a, b, dx, N);
 int l_w = l_w_global;
 a_global = a;
 b_global = b;
 dx_global = dx;
 N_global = N;
 //printf("\nPodaj liczbÄ™ wÄ…tkÃ³w: "); scanf("%d", &l_w);
 // tworzenie struktur danych do obsÅ‚ugi wielowÄ…tkowoÅ›ci
 pthread_t tid[l_w];
 int id[l_w];
 int i;
 // tworzenie wÄ…tkÃ³w
 for(i=0;i<l_w;i++)
 {
id[i]=i;
 }
 for(i=0;i<l_w;i++)
 {
pthread_create(&tid[i], NULL, calka_fragment_petli_w, &id[i]);
 }
 // oczekiwanie na zakoÅ„czenie pracy wÄ…tkÃ³w
 for(i=0;i<l_w;i++)
 {
pthread_join(tid[i],NULL);
 }
 return(calka_global);
}
void* calka_fragment_petli_w(void* arg_wsk){
 int my_id;
 my_id = *((int*)arg_wsk);
 double a, b, dx; // skÄ…d pobraÄ‡ dane a, b, dx, N, l_w ?
 int N, l_w; // wariant 1 - globalne
 
 // a = a_global; // itd. itp. - wartoÅ›ci globalne nadaje calka_zrownoleglenie_petli
 a = a_global;
 b = b_global;
 dx = dx_global;
 N = N_global;
 l_w = l_w_global;
 int my_start = my_id;
 int my_end = N;
 int my_stride = l_w;
 // something else ?
 
 //printf("%d", N);
 printf("\nWÄ…tek %d\n", my_id);
 printf("my_start %d, my_end %d, my_stride %d\n",
my_start, my_end, my_stride);
 int i;
 double calka = 0.0;
 for(i=my_start; i<my_end; i+=my_stride){
 double x1 = a + i*dx;
 calka += 0.5*dx*(funkcja(x1)+funkcja(x1+dx));
 //printf("i %d, x1 %lf, funkcja(x1) %lf, caÅ‚ka = %.15lf\n",
 // i, x1, funkcja(x1), calka);
 }
 
 pthread_mutex_lock(&muteks);
 calka_global += calka;
 pthread_mutex_unlock(&muteks);
 pthread_exit((void*)0);
}
void* calka_podobszar_w(void* arg_wsk);
double calka_dekompozycja_obszaru(double a, double b, double dx, int N){
 //printf("a %lf, b %lf, dx %lf, n %d\n", a, b, dx, N);
 int l_w = l_w_global;
 //printf("\nPodaj liczbÄ™ wÄ…tkÃ³w: "); scanf("%d", &l_w);
 double calka_suma_local = 0.0;
 // tworzenie struktur danych do obsÅ‚ugi wielowÄ…tkowoÅ›ci
 pthread_t tid[l_w_global];
 struct dane *tab;
 double obszar_watku = (b-a)/l_w_global;
 tab = (struct dane *)malloc(l_w_global*sizeof(struct dane));
 
 for(int i = 0; i<l_w_global; i++)
 {
 tab[i].a = obszar_watku*i;
 tab[i].b = tab[i].a+obszar_watku;
 tab[i].dx = 0.0001;
 tab[i].N = ceil((tab[i].b - tab[i].a)/tab[i].dx);
 tab[i].dx = (tab[i].b - tab[i].a)/tab[i].N;
 tab[i].id = i;
 }
 // tworzenie wÄ…tkÃ³w
 for(int i=0;i<l_w;i++)
 {
pthread_create(&tid[i], NULL, calka_podobszar_w, &tab[i]);
 }
 // oczekiwanie na zakoÅ„czenie pracy wÄ…tkÃ³w
 for(int i=0;i<l_w;i++)
 {
pthread_join(tid[i],NULL);
 }
 for(int i = 0; i<l_w_global;i++)
 {
 calka_suma_local+=tab[i].wynik;
 }
 return(calka_suma_local);
}
void* calka_podobszar_w(void* arg_wsk){
 double a_local, b_local, dx_local;
 int N_local;
 int my_id;
 struct dane *struktura = arg_wsk;
 a_local = struktura -> a;
 b_local = struktura -> b;
 dx_local = struktura -> dx;
 N_local = struktura -> N;
 my_id = struktura -> id;
 
 printf("\nWÄ…tek %d\n", my_id);
 printf("a_local %lf, b_local %lf, dx_local %lf, n_local %d\n",
a_local, b_local, dx_local, N_local);
 int i;
 double calka = 0.0;
 for(i=0; i<N_local; i++){
 double x1 = a_local + i*dx_local;
 calka += 0.5*dx_local*(funkcja(x1)+funkcja(x1+dx_local));
 //printf("i %d, x1 %lf, funkcja(x1) %lf, caÅ‚ka = %.15lf\n",
 //i, x1, funkcja(x1), calka);
 }
 
 struktura -> wynik = calka;
}
```

# **Sprawozdanie z laboratorium 6.**

13.11.2020, godz. 8:00 (piÄ…tek)

Wersja 2 - poprawiona.

### Cel:

Celem laboratorium byÅ‚o opanowanie **podstaw tworzenia wÄ…tkÃ³w** oraz **podstawowych metod synchronizacji** w Javie.

### Wykonanie:

- Pobrano ze strony przedmiotu PR plikÃ³w [Obraz.java](http://obraz.java) oraz Histogram_test.java.

Charakterystyka klasy **Obraz**:

- wszystkie atrybuty klasy sÄ… prywatne - dostÄ™p przez odp. metody

```jsx
		private int size_n;
    private int size_m;
    private char[][] tab;
    private int[] histogram;
```

Posiada dwa podstawowe elementy skÅ‚adowe:

- dwuwymiarowÄ… tablicÄ™ znakÃ³w (o zadanym przez uÅ¼ytkownika rozmiarze)

```jsx
	this.size_n = n;
	this.size_m = m;
	tab = new char[n][m];
```

- histogram odpowiadajÄ…cy tablicy â†’ przechowuje liczbÄ™ wystÄ…pieÅ„ kaÅ¼dego ze znakÃ³w w tablicy
- tablica jest wypeÅ‚niana za pomocÄ… podwÃ³jnej pÄ™tli po wszystkich wyrazach

```jsx
final Random random = new Random();
	
	for(int i=0;i<n;i++) {
	    for(int j=0;j<m;j++) {	
		tab[i][j] = (char)(random.nextInt(94)+33);  // ascii 33-127
		System.out.print(tab[i][j]+" ");
	    }
	    System.out.print("\n");
	}
	System.out.print("\n\n");

	histogram = new int[94];
   	clear_histogram();
```

- klasa Obraz posiada rÃ³wnieÅ¼ 3 funkcje wykonywane sekwencyjnie
1. Czyszczenie histogramu

```jsx
public void clear_histogram(){
	for(int i=0;i<94;i++) histogram[i]=0;
    }
```

2. Obliczanie histogramu

```jsx
public void calculate_histogram(){

	for(int i=0;i<size_n;i++) {
	    for(int j=0;j<size_m;j++) {
	
		for(int k=0;k<94;k++) {
		    if(tab[i][j] == (char)(k+33)) histogram[k]++;	    
}
```

3. Drukowanie histogramu

```jsx
public void print_histogram(){
	
	for(int i=0;i<94;i++) {
	    System.out.print((char)(i+33)+" "+histogram[i]+"\n");	    
}
```

- Dodatkowo mamy klasÄ™ Histogram_test:

```java
class Histogram_test {
    
    public static void main(String[] args) {

	Scanner scanner = new Scanner(System.in);
	
	System.out.println("Set image size: n (#rows), m(#kolumns)");
	int n = scanner.nextInt();
	int m = scanner.nextInt();
	Obraz obraz_1 = new Obraz(n, m);

	obraz_1.calculate_histogram();
	obraz_1.print_histogram();

	// System.out.println("Set number of threads");
	// int num_threads = scanner.nextInt();

	// Watek[] NewThr = new Watek[num_threads];
    
	// for (int i = 0; i < num_threads; i++) {
	//     (NewThr[i] = new Watek(...,obraz_1)).start();
	// }
    
	// for (int i = 0; i < num_threads; i++) {
	//     try {
	// 	NewThr[i].join();
	//     } catch (InterruptedException e) {}
	// }

    }

}

```

- NastÄ™pnie uruchomiono kod, sprawdzono poprawnoÅ›Ä‡  jego dziaÅ‚ania.
- Dokonano zmian dla wersji rÃ³wnolegÅ‚ej - dokonywanie obliczeÅ„ rÃ³wnolegle przy uÅ¼yciu wÄ…tkÃ³w Javy.
- WÄ…tki pracujÄ… przez wywoÅ‚ywanie odpowiednich metod klasy Obraz.
- Stworzono klasÄ™ Watek dziedziczÄ…cÄ… po Thread, ktÃ³ra przyjmuje jako atrybuty z konstuktora znak oraz obraz, metoda ktÃ³ra tutaj uruchamia liczy wystapienia jednego znaku i drukuje te wystapienia dla danego obrazu.

```java
public class Watek extends Thread {
    Obraz obraz; 
    int znak;
    
    public Watek(int znak_s, Obraz obraz){
        this.znak=znak_s;
        this.obraz=obraz;
    }

    public void run(){
        obraz.calculateOneChar(znak);
        obraz.printHistogramChar(znak);
    }
}

```

- klasÄ™ Obraz rozszerzono o dodatkowe metody, do liczenia wystapien danego znaku oraz do drukowania wystapien dla tego znaku - dziÄ™ki temu moÅ¼liwa byÅ‚a nastÄ™pnie praca wielowÄ…tkowa. Stworzono tu ilustracjÄ™ graficznÄ…, pokazujÄ…cÄ… uÅ¼ycie kaÅ¼dego znaku

```java
public void calculateOneChar(int k){

        for(int i=0;i<size_n;i++) {
            for(int j=0;j<size_m;j++) {
                if(tab[i][j] == (char)(k+33)) histogram[k]++;
            }
        }
    }

    public synchronized void printHistogramChar(int i){
        String znak=" ";
        Thread thread = Thread.currentThread();
        for(int k=0; k<histogram[i]; k++){
            znak+= "=";
        }
        System.out.print("WÄ…tek "+thread+": "+(char)(i+33)+" "+znak+"\n");
    }
```

- W klasie histogram odkomentowano linie odpowiedzialne za zrÃ³wnoleglenia kodu, dokonano edycji, utworzono tutaj funkcjÄ™ main ktÃ³ra byla wywoÅ‚ywana aby sprawdziÄ‡ efekty zrÃ³wnoleglenia - w klasie tej tworzymy tyle wÄ…tkÃ³w ile mamy rÃ³Å¼nych znakÃ³w w tablicy.
- ZrÃ³wnoleglenie to opiera siÄ™ na uÅ¼yciu wÄ…tkÃ³w Javy, tj.
    - tworzymy tablice obiektÃ³w Watek, ktÃ³re to przyjmujÄ… to strukturÄ™ naszej klasy dziedziczÄ…cej po Watek, majÄ…cej obraz oraz znak jako parametry.

```java
public static void main(String[] args) {

        Scanner scanner = new Scanner(System.in);

        System.out.println("Set image size: n (#rows), m(#columns)");
        int n = scanner.nextInt();
        int m = scanner.nextInt();
        Obraz obraz_1 = new Obraz(n, m);

        System.out.println("Set number of threads");
        int num_threads = scanner.nextInt();

        Watek[] NewThr = new Watek[num_threads];

        for (int i = 0; i < num_threads; i++) {
            (NewThr[i] = new Watek(i,obraz_1)).start();
        }

        for (int i = 0; i < num_threads; i++) {
            try {
                NewThr[i].join();
            } catch (InterruptedException e) {}
        }
    }
```

- efektem dziaÅ‚ania programu dostajemy liczbe uzycia kaÅ¼dego znaku, tak, Å¼e kaÅ¼dy znak jest liczony przez odpowiedni oddzielny wÄ…tek:
- tutaj generujemy 20x20 obraz o losowych znakach

![images/Untitled%205.png](images/Untitled%205.png)

- Wynik pokazujÄ…cy dziaÅ‚anie programu:

![images/Untitled%206.png](images/Untitled%206.png)

- Kolejnym zadaniem byÅ‚a modyfikacja napisanego programu w ten sposÃ³b, aby podzieliÄ‡ zadania na podzadania - uÅ¼yto tutaj tzw. **dekompozycji w dziedzinie problemu**.
    - dziedzina to zbiÃ³r znakÃ³w ASCII
    - dekompozycja przeprowadzana w sposÃ³b blokowy
- stworzono wiÄ™c na potrzeby Ä‡wiczenia klasÄ™ ktÃ³ra wykorzystuje interfejs Runnable i nazwano jÄ… tymczasowo KlasaRunnable:

```java
public class KlasaRunnable implements Runnable{
    private Obraz obraz;
    private int start, stop, id;

    public KlasaRunnable(Obraz obraz, int id, int start, int stop) {
        this.obraz = obraz;
        this.id = id;
        this.start = start;
        this.stop = stop;
    }

    @Override
    public void run() {
        for ( int i = start; i<stop; i++) {
            obraz.calculateOneChar(i);
            synchronized (obraz) {
                System.out.println("Watek: " + (id + 1) + "" );
                obraz.printHistogramChar(i);
            }
        }
    }
}
```

- w klasie tej dodatkowo nadajemy prywatne parametry charakteryzujÄ…ce start i stop, poza tym wykonanie jest bliÅºniaczo podobne do klasy z poprzedniego Ä‡wiczenia Watek.
- nastÄ™pnie zmodyfikowano klasÄ™ Histogram, poniÅ¼szy fragment tworzÄ…cy obraz zostawiono taki sam jak poprzednio: (w tej klasie mamy main naszego programu)
    - tworzenie scannera, wczytywanie liczby wÄ…tkÃ³w
    - tworzenie nowego obrazu o rozmiarze wczytanym z klawiatury

```java
				Scanner scanner = new Scanner(System.in);

        System.out.println("Set image size: n (#rows), m(#columns)");
        int n = scanner.nextInt();
        int m = scanner.nextInt();
        Obraz obraz_1 = new Obraz(n, m);

				System.out.println("Set number of threads");
        int threads = scanner.nextInt();
```

- lecz jako kolejne zostaÅ‚o wykonane:
    - deklarujemy nowÄ… tablicÄ™ wÄ…tkÃ³w o wielkoÅ›ci liczby wÄ…tkÃ³w pobranych z pliku
    - dzielimy liczbÄ™ znakÃ³w z tablicy ASCII przez liczbÄ™ wÄ…tkÃ³w
    - nadajemy start jako liczbÄ™ aktualnego iteratora pÄ™tli for razy liczbe znakÃ³w na wÄ…tek - przydzielamy znaki do wÄ…tkÃ³w, zgodnie z ich liczbÄ…
    - tworzymy wÄ…tki na bazie klasy runnable i pobranych parametrÃ³w
    - wywoÅ‚ujemy na wÄ…tku runnable metodÄ™ join()

```java
	
        int start;
        int stop;

        Thread[] ob_runnable = new Thread[threads];
        int charactersPerThread = (94/threads);

        for (int i = 0 ; i < threads; i++ ) {

            start = charactersPerThread*i;
            stop = charactersPerThread*(i+1);
            if(stop > 94) {
                stop = 94;
            }

            (ob_runnable[i] = new Thread(new KlasaRunnable(obraz_1, i, start, stop))).start();
        }

        for ( int i=0; i<threads;i++) {
            try {
                ob_runnable[i].join();
            }
            catch (InterruptedException e) {
                System.out.println("error");
            }
        }
```

Dla tej wersji Ä‡wiczenia powtarzamy prÃ³be wywoÅ‚ania w konsoli:

![images/Untitled%207.png](images/Untitled%207.png)

Dostajemy podziaÅ‚ ktÃ³e znaki majÄ… przetwarzaÄ‡ ktÃ³re wÄ…tki, i dostajemy wydrukowany wynik ile razy ktÃ³ry znak wystÄ…piÅ‚:

![images/Untitled%208.png](images/Untitled%208.png)

- jak widaÄ‡ wyniki sÄ… poprawne w obu wersjach programu

### **Wnioski:**

- wÄ…tek w Javie to obiekt klasy Thread. MoÅ¼na go znaleÅºÄ‡ w pakiecie java.lang
- wielowÄ…tkowoÅ›Ä‡ jest wbudowana w podstawy systemu, tj. w klasie Object moÅ¼na znaleÅºÄ‡ metody zwiÄ…zane z przetwarzaniem wspÃ³Å‚bieÅ¼nym wait, notify i notifyAll; kaÅ¼dy obiekt moÅ¼e siÄ™ staÄ‡ elementem przetwarzania wspÃ³Å‚bieÅ¼nego.

- **Sposoby uruchamiania wÄ…tkÃ³w w Javie:**
    - przekazanie obiektu implementujÄ…cego interfejs Runnable, ktÃ³rego metodÄ™ chcemy wywoÅ‚aÄ‡ jako argumnetu dla metody start obiektu klasy Thread (uruchamiana metoda musi implementowaÄ‡ metodÄ™ run)
    - poprzez uruchomienie metody start naszego obiektu(obiektu klasy pochodnej Thread, a metoda musi przeciÄ…Å¼aÄ‡ metodÄ™ run)
- oba powyÅ¼sze sposoby zostaÅ‚y przetestowane w wykonanym zadaniu

- klasa implementujÄ…ca interfejs Runnable moÅ¼e dziedziczyÄ‡ po innych klasach
- klasa dziedziczÄ…ca po Thread nie moÅ¼e dziedziczyÄ‡ naraz po innych klasach

- **klasa Thread posiada metody:**
    - void start() - polecenie do JVM Å¼eby uruchomiÄ‡ metodÄ™ run
    - void run() - uruchamia metodÄ™ run przekazanego obiektu implementujÄ…cego interfejs Runnable
    - static void sleep(long milis) - uÅ›pienie bieÅ¼Ä…cego wÄ…tku
    - void join(long milis), void join() - oczekiwanie na zakoÅ„czenie wÄ…tku
        - metoda join() klasy Thread to mechanizm wewnÄ™trznej synchronizacji. Gdy  jakiÅ› wÄ…tek wywoÅ‚uje tÄ… metodÄ™ na innym wÄ…tku, wywoÅ‚ujÄ…cy wÄ…tek przechodzi w stan oczekiwania dopÃ³ki wspomniany wÄ…tek zakoÅ„czy siÄ™. Metoda ta bÄ™dzie czekaÄ‡, na wspomniany wÄ…tek nawet jeÅ›li ten wÄ…tek bÄ™dzie zablokowany tudzieÅ¼ zajmie za duÅ¼o czasu, co moÅ¼e byÄ‡ problemem, bo wywoÅ‚ujÄ…cy wÄ…tek bÄ™dzie bez odpowiedzi. Aby rozwiÄ…zaÄ‡ ten problem moÅ¼na uÅ¼ywaÄ‡ przeciÄ…Å¼onych wersji join()
    - void interrput() - przerwanie dziaÅ‚Ä…nia wÄ…tku ( ustawienie sygnalizatora przerwania) - przerwanie tylko jeÅ›li wÄ…tek znajduje siÄ™ wewnÄ…trz metody ktÃ³rÄ… da siÄ™ przerwaÄ‡ np sleep, join, wait lub jeÅ›li w Å›rodku realizacji wÄ…tku wywoÅ‚ywana jest meotda static boolean interrupted() - ktÃ³ra sprawdza stan sygnalizatora przerwania - a nastÄ™pnie reaguje na prÃ³be przerwania, np zwraca wyjÄ…tek itp.

- **OkreÅ›lenie metod obiektu jako synchronizowanych czyni bezpiecznym jego dziaÅ‚anie,** wtedy:
    - jeÅ›li jest ona realizowana przez jakiÅ› wÄ…tek to uniemoÅ¼liwiona jest realizacja jakiejkolwiek synchronizowanej metody tego obiektu przez inne wÄ…tki
    - ten sam wÄ…tek MOÅ»E realizowaÄ‡ kolejne wywoÅ‚ania synchronizowanych metod
    - w momencie ZAKOÅƒCZENIA realizacji wszystkcih synchronizowanych metod obiektu przez dany wÄ…tek JVM wznawia dziaÅ‚anie pierwszego wÄ…tku oczekujÄ…cego w kolejce.

- **mechanizm synchronizacji w Javie opiera siÄ™ na istnieniu wewnÄ™trznych zamkÃ³w monitorowych** zwanych w skrÃ³cie monitorami.
    - kaÅ¼dy obiekt w Javie ma powiÄ…zany zamek monitorowy
    - wÄ…tek ktÃ³ry ROZPOCZYNA wykonywanie metody synchronizowanej ZAJMUJE zamek tego obiektu - dopÃ³ki go nie zwolni Å¼aden inny wÄ…tek nie ma do niego dostÄ™pu
    - ROZPOCZYNAJÄ„C wykonanie BLOKU SYNCHRONIZOWANEGO moÅ¼e rÃ³wnieÅ¼ zajÄ…Ä‡ zamek zwiÄ…zany z danym obiektem
    - ROZPOCZYNAJÄ„C wykonywanie STATYCZNEJ SYNCHRONIZOWANEJ FUNKCJI danej klasy, ZAJMUJE siÄ™ zamek zwiÄ…zany z obiektem reprezentujÄ…cym danÄ… klasÄ™.

- **synchronizowany blok oznacza siÄ™ okreÅ›leniem SYNCHRONIZED**
    - posiada argument, ktÃ³ry jest referencjÄ… do obiektu, ktÃ³rego zamek ma zostaÄ‡ zajÄ™ty
    - MoÅ¼naÂ tworzyÄ‡Â odrÄ™bneÂ obiekty,Â ktÃ³rychÂ jedynÄ…Â funkcjÄ… jest wystÄ™powanieÂ jakoÂ argumentÂ przyÂ synchronizacjiÂ blokÃ³w (peÅ‚niÄ… rolÄ™ rolÄ™ do MUTEKSÃ“W uÅ¼ywanych w linux/c)(w Javie nie mamy tylko metody trylock - moÅ¼na jÄ… dostaÄ‡ w pakiecie java.util.concurrency - jako **interfejs Lock** - posiada on standardowe funkcje muteksa i kilka jego implementacji tj. np ReentrantLock)

- **zaprezentowane mechanizmy sÄ… ukierunkowane na przetwarzanie wielowÄ…tkowe z maÅ‚Ä… liczbÄ… zadaÅ„.**
    - do masowej rÃ³wnolegÅ‚oÅ›ci moÅ¼emy uÅ¼yÄ‡ Javovskich mechanizmÃ³w takich jak **thread pool / executor**

# **Sprawozdanie z laboratorium 7.**

20.11.2020, godz. 8:00

### Cel:

Celem laboratorium byÅ‚o doskonalenie realizacji synchronizacji w jÄ™zyku C za pomocÄ… zmiennych warunku oraz w programach obiektowych w Javie za pomocÄ… narzÄ™dzi pakietu **java.util.concurrency**

### Wykonanie:

- utworzono katalog lab_7 i podkatalog lab_7_pthreads
- rozpakowano w nim paczkÄ™ CzytPis_Pthreads.tgz
- uruchomiono kod
- Przeanalizowano pseudokod monitora Czytelnia na slajdach z wykÅ‚adÃ³w oraz struktury kodu w paczce.
- NastÄ™pnie naprawiono bÅ‚Ä™dy w kodzie

Globalnie

- zainicjalizowano osobno **czytelnicyCond**  oraz **pisarzeCond -** sÄ… to zmienne typu pthread_cond_t

![images/Untitled%209.png](images/Untitled%209.png)

Zaimplementowano procedury interfejsu:

- **pthread_cond_wait** - sygnalizuje, czyli budzi pierwszy w kolejnoÅ›ci wÄ…tek oczekujÄ…cy "na zmiennej" *czytelnicyCond
- **pthread_cond_signal** rozgÅ‚asza sygnaÅ‚( budzi wszystkie wÄ…tki oczekujÄ…ce "na zmiennej" *czytelnicyCond)

![images/Untitled%2010.png](images/Untitled%2010.png)

- **pthread_cond_signal** rozgÅ‚asza sygnaÅ‚( budzi wszystkie wÄ…tki oczekujÄ…ce "na zmiennej" *psiarzeCond)

![images/Untitled%2011.png](images/Untitled%2011.png)

- **pthread_cond_wait** - sygnalizuje, czyli budzi pierwszy w kolejnoÅ›ci wÄ…tek oczekujÄ…cy "na zmiennej" *pisarzeCond

![images/Untitled%2012.png](images/Untitled%2012.png)

W ten sposÃ³b:

- nastÄ™puje Å›ledzenie liczby czytelnikÃ³w i pisarzy w czytelni
- podczas wchodzenia zwiÄ™kszamy licznik
- sprawdzane sÄ… warunki poprawnoÅ›ci aktualnych liczb pisarzy i czytelnikÃ³w
- przerywane jest dziaÅ‚anie w przypadku braku speÅ‚nienia warunkÃ³w
- kilka wÄ…tkÃ³w realizuje funkcje czytelnika i pisarza
- wykonywane jest to w nieskoÅ„czonej pÄ™tli
- wÄ…tki te realizujÄ… funkcje czytania i pisania i wzajemnie siÄ™ wykluczajÄ…

```java
#include<stdlib.h>
#include<stdio.h>
#include<unistd.h>
#include<pthread.h>

#include"czytelnia.h"

void *funkcja_czytelnika( void *);
void *funkcja_pisarza( void *);

int main(){
  
  int i;
  pthread_t pisarze[5], czytelnicy[10];
  int indeksy[10] = {0,1,2,3,4,5,6,7,8,9}; 
  czytelnia_t czytelnia;
  
  inicjuj(&czytelnia);
    
  for(i=0; i<5; i++){
    pthread_create( &pisarze[i], NULL, funkcja_pisarza, (void *)&czytelnia );
  }
  for(i=0; i<10; i++){
    pthread_create( &czytelnicy[i], NULL, funkcja_czytelnika, (void *)&czytelnia );
  }
  for(i=0; i<5; i++){
    pthread_join( pisarze[i], NULL); 
  }
  for(i=0; i<10; i++){
    pthread_join( czytelnicy[i], NULL ); 
  }
  
}

void *funkcja_czytelnika( void * arg){
  
  czytelnia_t* czytelnia_p = (czytelnia_t *)arg;
  
  for(;;){
    
    usleep(rand()%10000000);
    printf("czytelnik %d - przed zamkiem\n", pthread_self());
    
    my_read_lock_lock(czytelnia_p);
    
    // korzystanie z zasobow czytelni
    printf("czytelnik %d - wchodze\n", pthread_self());
    
    czytam(czytelnia_p);
    
    
    printf("czytelnik %d - wychodze\n", pthread_self());
    
    my_read_lock_unlock(czytelnia_p);
    
    printf("czytelnik %d - po zamku\n", pthread_self());
    
  }
  
}

void *funkcja_pisarza( void * arg){
  
  czytelnia_t* czytelnia_p = (czytelnia_t *)arg;
  
  for(;;){
    
    usleep(rand()%11000000);
    printf("pisarz %d - przed zamkiem\n", pthread_self());
    
    my_write_lock_lock(czytelnia_p);
    
    // korzystanie z zasobow czytelni
    printf("pisarz %d - wchodze\n", pthread_self());
    
    pisze(czytelnia_p);
    
    printf("pisarz %d - wychodze\n", pthread_self());
    
    my_write_lock_unlock(czytelnia_p);
    
    printf("pisarz %d - po zamku\n", pthread_self());
  }
  
}
```

![images/Untitled%2013.png](images/Untitled%2013.png)

### Wnioski:

- Problemem jest to, Å¼e nie ma dodatkowych warunkÃ³w, gdy jedna grupa procesÃ³w(pisarzy) modyfikuje zasÃ³b a druga grupa(czytelnikÃ³w) odczytuje stan zasobu. czyli pisaÄ‡ moÅ¼e tylko jeden wÄ…tek w jednym czasie, a czytaÄ‡ moÅ¼e wiele wÄ…tkÃ³w naraz
- Zmienne warunku to zmienne wspÃ³lne dla wÄ…tkÃ³w i sÅ‚uÅ¼Ä… one do identyfikacji uÅ›pionych wÄ…tkÃ³w.
- Nauczono siÄ™ tworzyÄ‡ zmienne typu pthrad_cond_t i uÅ¼ywanie ich jako argumentÃ³w do funkcji pthread_cond_wait,  pthread_cond_signal

# **Sprawozdanie z laboratorium 8.**

27.11.2020, godz. 8:00

### Cel:

Celem laboratorium byÅ‚o nabycie umiejÄ™tnoÅ›ci z pisania programÃ³w w jÄ™zyku Java z wykorzystaniem puli wÄ…tkÃ³w.

### Wykonanie:

- utworzenie katalogu roboczego
- napisanie sekwencyjnego programu obliczania caÅ‚ki korzystajÄ…c z dostarczonej klasy Calka_callable:

```java
package com.agh.pr.lab8;

public class Wykon {

    public static double x1=0;
    public static double x2= Math.PI;
    public static double dx = 0.001;

    public static void main(String[] args) {
        Calka_callable calka = new Calka_callable(x1,x2,dx);
        System.out.println("Wynik: = " + calka.compute());
    }
}
```

**Calka_callable** oblicza caÅ‚kÄ™ w zadanym przedziale, z granicami przedziaÅ‚u i dokÅ‚adnoÅ›ciÄ…
zadanÄ… w konstruktorze

- nastÄ™pnie rozpakowano katalog z plikami Java_Executor_test.tgz
- przeanalizowano pliki
- na podstawie przykÅ‚adu z wykorzystaniem **ExecutorService** oraz przy uÅ¼yciu klasy **Executors**  zmodyfikowano program tak, Å¼e:
    - uÅ¼ywana jest staÅ‚a pula wÄ…tkÃ³w
    - pojedyncze zadanie dla obiektu typu Executor stanowi obliczenie caÅ‚ki w podprzedziale,
    - w moim przykÅ‚adzie przedziaÅ‚ to 0 do PI wiÄ™c dzielimy koniec przedziaÅ‚u na liczbÄ™ taskÃ³w
    - stanowi to wielkoÅ›Ä‡ jednego podprzedziaÅ‚u do obliczeÅ„
    - zadania sÄ… przekazywane w jednej pÄ™tli
    - wyniki sÄ… odbieranie w kolejnej pÄ™tli

```java
package com.agh.pr.lab8;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;

public class Main {

    private static final int NTHREADS = 10;
    private static final int NTASKS = 20;

    public static void main(String[] args) {

        //WYKORZYSTANIE EXECUTOR SERVICE , EXECUTORS
        ExecutorService executor = Executors.newFixedThreadPool(NTHREADS);

        List<Future<Double>> list = new ArrayList<>();

        double interval = Math.PI/NTASKS;
        double calka = 0.0;

        //TWORZENIE WYNIKOW
        for (int i=0;i<NTASKS;i++) {
            Callable callable = new Calka_callable(interval*i,interval*(i+1),0.0001);
            Future<Double> future = executor.submit(callable);
            list.add(future);
        }

        //OODBIOR W KOLEJNEJ PETLI
        for(Future<Double> future_double:list) {
            try {
                calka += future_double.get();
            } catch (InterruptedException | ExecutionException e) {
                e.printStackTrace();
            }
        }
        //ZAKONCZENIE DZIALANIA EXECUTOR
        executor.shutdown();
        //WYDRUK CALKI
        System.out.println("Calka konc: " + calka);
    }
}
```

- klasa Executors dostarcza metody wytwÃ³rcze dla pul wÄ…tkÃ³w
- ExecutorService jest rozszerzeniem interfejsu Executor

![images/Untitled%2014.png](images/Untitled%2014.png)

![images/Untitled%2015.png](images/Untitled%2015.png)

### Wnioski:

- liczba wÄ…tkÃ³w i zadaÅ„ sÄ… niezaleÅ¼ne od siebie, jednak zwykle liczba zadaÅ„ jest wiÄ™ksza od liczby wÄ…tkÃ³w
- liczba wÄ…tkÃ³w jest zwiÄ…zana z liczbÄ… rdzeni procesora
- rÃ³wnolegÅ‚e dziaÅ‚ania problemu jest moÅ¼liwe dziÄ™ki temu, Å¼e uÅ¼ywamy jednej pÄ™tli z przekazywaniem zadaÅ„ a drugiej do odbierania.
- **executor** - interfejs wykonawcÃ³w, razem z pulÄ… wÄ…tkÃ³w **thread pool** to specjalne udogodnienia do masowej rÃ³wnolegÅ‚oÅ›ci

# **Sprawozdanie z laboratorium 9.**

4.12.2020, godz. 8:00 (piÄ…tek)

Wersja 2 - poprawiona.

### Cel:

- Celem tego laboratorium byÅ‚o nabycie umiejÄ™tnoÅ›ci **tworzenia i implementacji
programÃ³w rÃ³wnolegÅ‚ych z wykorzystaniem OpenMP.**

### Wykonanie:

- utworzono katalog roboczy lab_9
- skopiowano plik openmp_petle_simple.c

zawartoÅ›Ä‡ pliku:

```java
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <omp.h>

#define WYMIAR 13

main ()
{
  double a[WYMIAR];
  int n,i;

  for(i=0;i<WYMIAR;i++) a[i]=1.02*i;

  n=WYMIAR;

  double suma=0.0;
  for(i=0;i<WYMIAR;i++) {
      suma += a[i];
  }
  
  printf("Suma wyrazÃ³w tablicy: %lf\n", suma);

  double suma_parallel=0.0;
  // ...
  for(i=0;i<WYMIAR;i++) {
    int id_w = omp_get_thread_num();
    // ...
      suma_parallel += a[i];
      // ...
#pragma omp ordered
      printf("a[%2d]->W_%1d  ",i,id_w); 
  }

  printf("\nSuma wyrazÃ³w tablicy rÃ³wnolegle: %lf\n", suma_parallel);

}
```

- plik zawiera pÄ™tle sumujÄ…cÄ… wyrazy w tablicy w sposÃ³b nierÃ³wnolegÅ‚y, oraz schemat dziaÅ‚ania sumowanie w sposÃ³b rÃ³wnolegÅ‚y
- program zostaÅ‚ skompilowany komendÄ…:

```bash
gcc -fopenmp openmp_petle_simple.c -o openmp_petle_simple
```

- wynik dziaÅ‚ania programu:

![images/Untitled%2016.png](images/Untitled%2016.png)

- nastÄ™pnie zrÃ³wnolegliÅ‚em wskazanÄ… pÄ™tle for za pomocÄ… dyrektywy parallel:

![images/Untitled%2017.png](images/Untitled%2017.png)

- uÅ¼yto klauzuli default(none) aby wymusiÄ‡ jawne ustalenie charakteru zmiennych
- klauzulÄ™ ordered uÅ¼yto jedynie w celu wymuszenia kolejnoÅ›ci wykonywanych operacji, normalnie nie byÅ‚oby potrzebne
- program uruchomiono dla 4 wÄ…tkÃ³w
- liczba wÄ…tkÃ³w zostaÅ‚a ustawiona jako zmienna Å›rodowiskowa poza kodem programu
- wynik dziaÅ‚ania programu:

![images/Untitled%2018.png](images/Untitled%2018.png)

- wynik jest taki sam jak wynik niezrÃ³wnoleglony, co mÃ³wi o prawidÅ‚owym wykonaniu zrÃ³wnoleglenia - nie mierzymy czasu tylko jakoÅ›Ä‡, a tutaj jakoÅ›Ä‡ jest identyczna.
- dziÄ™ki uÅ¼yciu 4 wÄ…tkÃ³w widaÄ‡ jasny przydziaÅ‚, ktÃ³ry wyraz jest dodawany przez ktÃ³ry wÄ…tek W_0, W_1, W_2, W_3. (18 iteracji ogÃ³Å‚em)
- nastÄ™pnie przetestowano moÅ¼liwoÅ›ci z uÅ¼ycia klauzuli **schedule**
    - statycznie, rozmiar porcji: 3

```bash
schedule(static,3)
```

![images/Untitled%2019.png](images/Untitled%2019.png)

- statycznie, domyÅ›lny rozmiar porcji

```bash
schedule(static)
```

![images/Untitled%2020.png](images/Untitled%2020.png)

- statycznie, rozmiar porcji: 2

```bash
schedule(dynamic,2)
```

![images/Untitled%2021.png](images/Untitled%2021.png)

- dynamicznie, rozmiar porcji domyÅ›lny

```bash
schedule(dynamic)
```

![images/Untitled%2022.png](images/Untitled%2022.png)

- nastÄ™pnie dokonano analizy wynikÃ³w:
    - w statycznym przydziale widzimy podobny ale inny przydziaÅ‚ w wersji domyÅ›lnej i rozmairze porcji 3
    - w dynamicznym za kaÅ¼dym raze domyÅ›lnie pierwszy wolny wÄ…tek dostaje wszystkie zadania, moÅ¼e siÄ™ on jednak wielokrotnie odwoÅ‚ywaÄ‡ o przydziaÅ‚ wÄ…tku, stÄ…d prawdopodobnie caÅ‚y czas jeden ten sam wÄ…tek wykonuje caÅ‚Ä… robotÄ™.
- nastÄ™pnie skopiowano ze strony internetowej plik **openmp_petle.c**

```c
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <omp.h>

#define WYMIAR 13

main ()
{
  double a[WYMIAR][WYMIAR];
  int n,i,j;

  for(i=0;i<WYMIAR;i++) for(j=0;j<WYMIAR;j++) a[i][j]=1.02*i+1.01*j;

  n=WYMIAR;

  double suma=0.0;
  for(i=0;i<WYMIAR;i++) {
    for(j=0;j<WYMIAR;j++) {
      suma += a[i][j];
    }
  }
  
  printf("Suma wyrazÃ³w tablicy: %lf\n", suma);

  omp_set_nested(1);

  double suma_parallel=0.0;
  // ...
  for(i=0;i<WYMIAR;i++) {
    int id_w = omp_get_thread_num();
    // ...
    for(j=0;j<WYMIAR;j++) {
      suma_parallel += a[i][j];
      // ...
      printf("(%2d,%2d)-W(%1d,%1d) ",i,j,id_w,omp_get_thread_num()); 
    }
    // ...
    printf("\n");
  }

  printf("Suma wyrazÃ³w tablicy rÃ³wnolegle: %lf\n", suma_parallel);

}
```

- plik ten jest bardziej rozbudowanym poprzednim plikiem, zawiera drugi wymiar.
- uruchomiono program z liczbÄ… wÄ…tkÃ³w 3
- wynik dziaÅ‚ania programu:

![images/Untitled%2023.png](images/Untitled%2023.png)

- dostosowano rozmiar tablicy: na 11:

![images/Untitled%2024.png](images/Untitled%2024.png)

Suma wyrazÃ³w w typ wypadku 1228.15

- nastÄ™pnie zrÃ³wnoleglono pÄ™tle zewnÄ™trznÄ… czyli zostaÅ‚a wykonana tzw. **dekompozycja wierszowa**
    - suma jest uzyskiwana przez klauzulÄ™ reduction
    - schedule static, rozmiar porcji 2

![images/Untitled%2025.png](images/Untitled%2025.png)

- wynik dziaÅ‚ania:

![images/Untitled%2026.png](images/Untitled%2026.png)

- jak widaÄ‡ wyniki sÄ… poprawne,
- widaÄ‡ ciekawy schemat przydziaÅ‚u wÄ…tkÃ³w do zadaÅ„, caÅ‚e obliczenia stanowiÄ… obszar rÃ³wnolegÅ‚y
- nastÄ™pnie dokonano zrÃ³wnoleglenia pÄ™tli wewnÄ™trznej tzw. **dekompozycji kolumnowej**

![images/Untitled%2027.png](images/Untitled%2027.png)

- wynik dziaÅ‚ania:

![images/Untitled%2028.png](images/Untitled%2028.png)

- zbliÅ¼enie:

![images/Untitled%2029.png](images/Untitled%2029.png)

![images/Untitled%2030.png](images/Untitled%2030.png)

- znÃ³w obserwujemy ciekawy podziaÅ‚ zrÃ³wnoleglenia: przechodzenie miÄ™dzy kolumnami oznacza wchodzenie w kolejne to obszary rÃ³wnolegÅ‚e.
- **dekompozycja blokowa:**
    - tutaj odpowiednio sterujÄ™ liczbÄ… wÄ…tkÃ³w, tj. 3 i 2 kolejno w wierszach i kolumnach
    - kaÅ¼da pÄ™tla zrÃ³wnoleglana jest wÅ‚asnym obszarem rÃ³wnolegÅ‚ym przez uÅ¼ycie **#pragma omp parralel for**

![images/Untitled%2031.png](images/Untitled%2031.png)

wynik:

![images/Untitled%2032.png](images/Untitled%2032.png)

zbliÅ¼enie:

![images/Untitled%2033.png](images/Untitled%2033.png)

![images/Untitled%2034.png](images/Untitled%2034.png)

- obserwujemy znÃ³w poprawnie obliczona sumÄ™ oraz blokowe rozÅ‚oÅ¼enie obszarÃ³w rÃ³wnolegÅ‚ych.

**Wnioski**:

- klauzula schedule pozwala na podziaÅ‚ zadaÅ„ miedzy wÄ…tki w zaleÅ¼noÅ›ci od woli programisty
- w dynamicznej wersji schedule za kaÅ¼dym razem domyÅ›lnie pierwszy wolny wÄ…tek dostaje wszystkie zadania
- moÅ¼e siÄ™ on jednak wielokrotnie odwoÅ‚ywaÄ‡ o przydziaÅ‚ wÄ…tku, co jest prawdopodobnym powodem tego, Å¼e caÅ‚y czas ten sam wÄ…tek wykonuje wsyzstkie zadania
- klauzula #pragma omp barrier pozwala na wstrzymywanie wÄ…tkÃ³w do momentu, aÅ¼ inne wÄ…tki napotkajÄ… rÃ³wnieÅ¼ tÄ… dyrektywÄ™
- klauzula #pragma omp ordered pozwala na wykonywanie rÃ³wnolegÅ‚ej pÄ™tli tak jakby byÅ‚a sekwencyjna, nie jest to do uÅ¼ywania w normalnych programach ale pozwala na efektywnÄ… analizÄ™ wynikÃ³w poprzez to co zostanie wydrukowane w terminalu
- wÄ…tki moÅ¼na okreÅ›laÄ‡ przez zmiennÄ… Å›rodkowiskowÄ… OMP_NUM_THREADS=liczba_watkow, uzywajac funkcji omp_set_num_threads(liczba_watkÃ³w) albo w samej klauzuli dopisaÄ‡ moÅ¼na num_threads(liczba_watkow)
- dziÄ™ki klauzuli reduction nie trzeba ustawiaÄ‡ zmiennych prywatnych jako private oraz nie trzeba uÅ¼ywaÄ‡ #pragma critical przed wejÅ›ciem do sekcji krytycznej

# **Sprawozdanie z laboratorium 10.**

11.12.2020, godz. 8:00

**Cel:**

- pogÅ‚Ä™bienie umiejÄ™tnoÅ›ci pisania programÃ³w rÃ³wnolegÅ‚ych w Å›rodowisku OpenMP

**Wykonanie:**

- Utworzenie katalogu roboczego PR_lab10
- Skopiowanie paczki openmp_watki_zmienne.tgz, rozpakowanie w katalogu roboczym i uruchomienie programu.
- Poprawienie czytelnoÅ›ci wydruku, tak Å¼e:
    - kaÅ¼dy wÄ…tek drukuje bez ingerencji innych
    - wydruki zmiennych wspÃ³lnych w kaÅ¼dym wÄ…tku byÅ‚ takie same jak wydruki po wyjÅ›ciu z obszaru rÃ³wnolegÅ‚ego
- Sprawdzenie poprawnoÅ›ci dziaÅ‚ania na domyÅ›lnej liczbie wÄ…tkÃ³w, a nastÄ™pnie ustawienie 33 wÄ…tkÃ³w przez funkcjÄ™ **omp_set_num_threads(33)**

    ![images/Untitled%2035.png](images/Untitled%2035.png)

- analiza programu oraz usuniÄ™cie zaleÅ¼noÅ›ci

![images/Untitled%2036.png](images/Untitled%2036.png)

- nastÄ™pnie napisano drugi obszar rÃ³wnolegÅ‚y. UÅ¼yto dyrektywy threadprivate - zmienne objÄ™te tÄ… dyrektywÄ… majÄ… zachowaÄ‡ swoje prywatne wartoÅ›ci

![images/Untitled%2037.png](images/Untitled%2037.png)

- zmienna f_threadprivate dostaje wartoÅ›Ä‡ liczby wÄ…tkÃ³w

![images/Untitled%2038.png](images/Untitled%2038.png)

![images/Untitled%2039.png](images/Untitled%2039.png)

- nastÄ™pnie wypisanie tej wartoÅ›ci w drugim obszarze

![images/Untitled%2040.png](images/Untitled%2040.png)

![images/Untitled%2041.png](images/Untitled%2041.png)

### Wnioski:

- zaleÅ¼noÅ›ci to wzajemne uzaleÅ¼nienie od siebie instrukcji ktÃ³re nakÅ‚ada ograniczenia na kolejnoÅ›Ä‡ ich realizacji, rozrÃ³Å¼niamy:
    - zaleÅ¼noÅ›ci zasobÃ³w: wiele wÄ…tkÃ³w jednoczeÅ›nie usiÅ‚uje korzystaÄ‡ z wybranego zasobu
    - zaleÅ¼noÅ›ci sterowania: wykonanie danej instrukcji zaleÅ¼y od rezultatÃ³w poprzedzajÄ…cych instrukcji warunkowych
    - zaleÅ¼noÅ›ci danych: instrukcje wykonywane w bezpoÅ›rednim sÄ…siedztwie czasowym operujÄ… na tych samych danych + przynajmniej jedna z instrukcji dokonuje zapisu
- omp barrier - to dyrektywa ktÃ³ra identyfikuje punkt synchronizacji, w ktÃ³rym wÄ…tki - w rÃ³wnolegÅ‚bym regionie - bÄ™dÄ… czekaÄ‡, aÅ¼ wszystkie inne wÄ…tki osiÄ…gnÄ… ten punkt. PÃ³Åºniej poza tym punktem instrukcje wykonywane sÄ… rÃ³wnolegle.
- omp ciritcal -to dyrektywa ktÃ³ra identyfikuje sekcjÄ™ kodu ktÃ³ra musi byÄ‡ wykonywana jednoczeÅ›nie przez pojedynczy wÄ…tek
- omp atomic - pozwala na atomiczny dostÄ™p do konkretnego miejscia w pamiÄ™ci
- dziÄ™ki omp atomic moÅ¼liwe jest unikniÄ™cie warunkÃ³w wyÅ›cigu dziÄ™ki bezpoÅ›redniej kontroli kontroli wspÃ³Å‚bieÅ¼nych wÄ…tkÃ³w, ktÃ³re mogÄ… odczytywaÄ‡ lub zapisywaÄ‡ do lub z okreÅ›lonego miejsca w pamiÄ™ci. Tak wiÄ™c dziÄ™ki omp atomic moÅ¼na pisaÄ‡ wydajniejsze algorytmy wspÃ³Å‚bieÅ¼ne z mniejszÄ… liczbÄ… blokad.

# **Sprawozdanie z laboratorium 11.**

17.12.2020, godz. 8:00

Wersja 2. Poprawiona. 

**Cele:**

- pogÅ‚Ä™bienie umiejÄ™tnoÅ›ci pisania programÃ³w rÃ³wnolegÅ‚ych w Å›rodowisku OpenMP
- wykorzystanie puli wÄ…tkÃ³w - zadaÅ„ OpenMP

**Wykonanie:**

- Utworzenie katalogu roboczego PR_lab11 i podkatalogu search_max_openmp
- Rozpakowano paczkÄ™ ze strony [http://ww1.metal.agh.edu.pl/~banas/PR](http://ww1.metal.agh.edu.pl/~banas/PR/RR_L11_openmp_tasks.pdf)
- Przygotowano skrypt **Makefile**
- Uruchomiono program.

    wynik dziaÅ‚ania:

    ![images/Untitled%2042.png](images/Untitled%2042.png)

    - nastÄ™pnie uzupeÅ‚niono program o definicjÄ™ zadaÅ„ - tasks dla wersji rÃ³wnolegÅ‚ej openmp wyszukiwania liniowego

    ```c
    double task_maxes_tab[num_tasks];
    ```

    - do dyrektywy #pragma omp task dopisano dyrektywÄ™ **default(none),** ktÃ³ra zmusza programistÄ™ do  ****i ustalono ktÃ³re zmienne naleÅ¼y przekazaÄ‡ w dyrektywie **firstprivate**, sÄ… one wtedy inicjalizowane z wartoÅ›ciÄ… ktÃ³re miaÅ‚y zanim weszÅ‚y w rÃ³wnolegÅ‚y region
    - If i is made firstprivate, then it is initialised with the value that i

    ![images/Untitled%2043.png](images/Untitled%2043.png)

    wywoÅ‚anie:

    ![images/Untitled%2044.png](images/Untitled%2044.png)

    - nastÄ™pnie pobrano plik openmp_sortowanie.tgz, rozpakowano i uruchomiono:

    ![images/Untitled%2045.png](images/Untitled%2045.png)

    - dokonano analizy technik zrÃ³wnoleglenia sortowania przez scalanie (plik merge_sort_openmp.c - analiza), omÃ³wienie uÅ¼ytych dyrektyw

    ```c
    #pragma omp task final( poziom>max_poziom ) default(none) firstprivate(A,p,r,q1,poziom)
    ```

    - **task** tworzy zadania wykorzystywane pÃ³Åºniej przez wÄ…tki. RÃ³Å¼niÄ… siÄ™ tym od sekcji, Å¼e sÄ… kolejkowane i wykonywane gdy tylko siÄ™ da, w tzw. task scheduling points
    - **final** oznacza, Å¼e gdy koÅ„cowe wyraÅ¼enie w klauzuli ma wartoÅ›Ä‡ true, kaÅ¼dy wygenerowany task bÄ™dzie ostatecznym taskiem, bÄ™dzie wykonywany od razu. Wszystkie konstrukty spotkane podczas wykonania tego taska wygenerujÄ… rÃ³wnieÅ¼ ostateczne, wykonywane od razu taski.
    - **default(none)** - omÃ³wione wczeÅ›niej, narzucenie, Å¼e programista musi wybraÄ‡ czy kaÅ¼da zmienna jest shared czy private
    - **firstprivate -** zmienne przekazywane tak ****sÄ… inicjalizowane z wartoÅ›ciÄ… ktÃ³re miaÅ‚y zanim weszÅ‚y w rÃ³wnolegÅ‚y region

    ```c
    #pragma omp taskwait
    ```

- **taskwait** dziaÅ‚a jak *barrier*, ale dla taskÃ³w - upewnia siÄ™ Å¼e bieÅ¼Ä…ce wykonanie zostanie zatrzymane zanim wszystkie zakolejkowane taski zostanÄ… wykonane

```c
#pragma omp parallel sections default(none) firstprivate(A,p,r,q1)
```

```c
#pragma omp section
```

- **section -** jest to jednostka podziaÅ‚u kodu, ktÃ³ry jest potem rozdystrybuowany i wykonywany przez wÄ…tki, kaÅ¼dy blok jest wykonywany jeden raz przez jeden z wÄ…tkÃ³w z zbioru wÄ…tkÃ³w w kontekÅ›cie jego taska.
- **sections -** to znak, Å¼e tu bÄ™dzie podziaÅ‚ na sections, sÅ‚uÅ¼y tutaj teÅ¼ jako oddzielenie tej czÄ™Å›ci kodu

- nastÄ™pnie w pliku edytowanym wczeÅ›niej dopisano odpowiednie dyrektywy (definicje zadaÅ„) dla wersji rÃ³wnolegÅ‚ej openmp wyszukiwania binarnego
- uÅ¼yto tutaj rÃ³wnieÅ¼ default(none) zgodnie z wnioskami, ktÃ³re wczeÅ›niej wyciÄ…gnÄ…Å‚em.

![images/Untitled%2046.png](images/Untitled%2046.png)

-wynik wywoÅ‚ania:

![images/Untitled%2047.png](images/Untitled%2047.png)

### Wnioski:

- **Zmienne w rÃ³wnolegÅ‚ym regionie OpenMP mogÄ… byÄ‡ zarÃ³wno shared albo private.**
    - JeÅ›li zmienna jest **shared**, to istnieje jedna instancja ten zmiennej ktÃ³ra jest przekazywana miÄ™dzy wszystkimi wÄ…tkami.
    - JeÅ›li zmienna jest **private** to kaÅ¼dy wÄ…tek w zbiorze wÄ…tkÃ³w trzyma prywatnÄ… kopiÄ™ tej zmiennej
    - Zmienne zadeklarowane poza obszarem rÃ³wnolegÅ‚ym, sÄ… generalnie shared w rÃ³wnolegÅ‚ym obszarze, chyba Å¼e sÄ… to zmienne iteracyjne pÄ™tli. Wtedy sÄ… private.
    - Zmienne zadeklarowane w rÃ³wnolegÅ‚ym obszarze sÄ… prywatne.
- **Sensem uÅ¼ywania default(none) jest oznaczenie, Å¼e to programista musi dokonaÄ‡ podziaÅ‚u ktÃ³re zmienne sÄ… shared, a ktÃ³re private**
    - dziÄ™ki temu nieuwaÅ¼ny programista nie popeÅ‚ni bÅ‚Ä™du polegajÄ…cego na braku oznaczenia jaka jest dana zmienna. **UÅ¼ywanie defualt(none) jest zalecanÄ… praktykÄ… zwiÄ™kszajÄ…cÄ… niezawodnoÅ›Ä‡ pisanego kodu.**

- DobrÄ… praktykÄ… jest deklarowanie prywatnych zmiennych w Å›rodku rÃ³wnolegÅ‚ych obszarÃ³w, gdzie siÄ™ tylko da. DziÄ™ki temu kod jest czytelniejszy.
- Taski sÄ… kolejkowane i wykonywane kiedy to moÅ¼liwe w tak zwanych "task scheduling points".
- Pod pewnymi warunkami, moÅ¼na przenieÅ›Ä‡ task pomiÄ™dzy wÄ…tkami, nawet w czasie ich trwania - takie taski sÄ… nazywane *untied*.
- dyrektywa single pozwala na wyrÃ³Å¼nienie sekcji kodu do wykonania w kolejnym wÄ…tku

[https://stackoverflow.com/questions/15304760/how-are-firstprivate-and-lastprivate-different-than-private-clauses-in-openmp](https://stackoverflow.com/questions/15304760/how-are-firstprivate-and-lastprivate-different-than-private-clauses-in-openmp)

[http://jakascorner.com/blog/2016/07/omp-default-none-and-const.html#:~:text=The default(none) clause forces,clearer and has less bugs](http://jakascorner.com/blog/2016/07/omp-default-none-and-const.html#:~:text=The%20default(none)%20clause%20forces,clearer%20and%20has%20less%20bugs).

[http://jakascorner.com/blog/2016/06/omp-data-sharing-attributes.html](http://jakascorner.com/blog/2016/06/omp-data-sharing-attributes.html)

# **Sprawozdanie z laboratorium 12.**

08.01.2020, godz. 8:00

Wersja 2. Poprawiona.

**Cele:**

- pogÅ‚Ä™bienie umiejÄ™tnoÅ›ci pisania programÃ³w rÃ³wnolegÅ‚ych w Å›rodowisku OpenMP
- wykorzystanie puli wÄ…tkÃ³w - zadaÅ„ OpenMP

**Wykonanie:**

- Utworzono katalog roboczy lab12 i podkatalog simple.
- Skopiowano plik MPI_simple.tgz ze strony przedmiotu. Rozpakowano i zmodyfikowano makefile, skompilowano, wynik:

![images/Untitled%2048.png](images/Untitled%2048.png)

- dokonano edycji w kodzie pliku
    - utworzono tablice address_sender, address_received o wielkoÅ›ci 256, utworzono zmiennÄ… length
    - dodano funkcjÄ™ gethostname(&address_sender, 256);
    - dokonano zmian MPI_INT na MPI_CHAR zgodnie z zadeklarownaymi tablicami
    - dokonano zmian na address_sender i address_received w kolejno MPI_Send i MPI_Recv
    - zmieniono %d na %s do poprawnego wyÅ›wietlania

![images/Untitled%2049.png](images/Untitled%2049.png)

![images/Untitled%2050.png](images/Untitled%2050.png)

- utworzono podkatalog sztafeta:
- na podstawie wnioskÃ³w z poprzedniego zadania opracowano program propagujÄ…cy komunikaty w konwencji pierÅ›cienia (sztafeta).
- KaÅ¼dy proces ustala:
    1. jednego poprzednika (otrzymuje od niego komunikaty
    2. jednego nastÄ™pcÄ™ (przekazuje mu komunikaty)
- Zaimplementowano rozwiÄ…zanie:
    1. Wersja 1. Ostatni proces koÅ„czy sztafetÄ™

```c
#include <stdlib.h>
#include<stdio.h>
#include<math.h>
#include "mpi.h"
int main(int argc, char** argv) {

	int rank, ranksent, size, source, dest, tag, i;
	MPI_Status status;

	MPI_Init(&argc, &argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	
	 if( rank == 0 ){
		 dest=1;
		 tag=0;
	
		 MPI_Send( &rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD );
		 printf("[proces %d] wyslal [%d] do [proces %d]\n", rank, rank, dest);
		 	
	 } else if(rank == size-1){
		 	
		 dest=0;
		 tag=0;
	
		 MPI_Recv( &ranksent, 1, MPI_INT, rank-1, tag, MPI_COMM_WORLD, &status);
		 printf("[proces %d] odebral [%d] od [proces %d]\n", rank, ranksent, status.MPI_SOURCE);
		 	
	 } else {
		 	
		 dest=rank+1;
		 tag=0;
		 	
		 MPI_Recv( &ranksent, 1, MPI_INT, rank-1, tag, MPI_COMM_WORLD, &status);
		 printf("[proces %d] odebral [%d] od [proces %d]\n", rank, ranksent, status.MPI_SOURCE);
	
		 MPI_Send( &rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD );
		 printf("[proces %d] wyslal [%d] do [proces %d]\n", rank, rank, dest);
	 }

	MPI_Finalize();
	return(0);
}
```

- wynik dziaÅ‚ania programu:

![images/Untitled%2051.png](images/Untitled%2051.png)

- Jak widaÄ‡ proces 3 po odoebraniu koÅ„czy wykonanie

2 wersja: Ostatni proces przesyÅ‚a dane do pierwszego procesu: tzw. **zamkniÄ™ty pierÅ›cieÅ„:**

```c
#include <stdlib.h>
#include<stdio.h>
#include<math.h>
#include "mpi.h"
int main(int argc, char** argv) {

	int rank, ranksent, size, source, dest, tag, i;
	MPI_Status status;

	MPI_Init(&argc, &argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);

	   
		if(size>1){
			if( rank == 0 ){
				dest=1;
				tag=0;

				MPI_Send( &rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD );
				printf("[proces %d] wyslal [%d] do [proces %d]\n", rank, rank, dest);
				
				MPI_Recv( &ranksent, 1, MPI_INT, rank-1, tag, MPI_COMM_WORLD, &status);
				printf("[proces %d] odebral [%d] od [proces %d]\n", rank, ranksent, status.MPI_SOURCE);
			} else if(rank == size-1){
				
				dest=0;
				tag=0;

				MPI_Recv( &ranksent, 1, MPI_INT, rank-1, tag, MPI_COMM_WORLD, &status);
				printf("[proces %d] odebral [%d] od [proces %d]\n", rank, ranksent, status.MPI_SOURCE);
				MPI_Send( &rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD );
				printf("[proces %d] wyslal [%d] do [proces %d]\n", rank, rank, dest);
			} else {
				
				dest = rank + 1;
				tag=0;
				MPI_Recv( &ranksent, 1, MPI_INT, rank-1, tag, MPI_COMM_WORLD, &status);
				printf("[proces %d] odebral [%d] od [proces %d]\n", rank, ranksent, status.MPI_SOURCE);
				MPI_Send( &rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD );
				printf("[proces %d] wyslal [%d] do [proces %d]\n", rank, rank, dest);
			}
		} else{
	   printf("Pojedynczy proces o randze: %d (brak komunikatÃ³w)\n", rank);
		}

	MPI_Finalize();

	return(0);
}
```

![images/Untitled%2052.png](images/Untitled%2052.png)

- uruchomienie obu wariantÃ³w wizualizuje przepÅ‚yw odbieranie/wysyÅ‚anie liczby pomiÄ™dzy procesami.
- nastÄ™pnie utworzono podkatalog struktura:
- zaprojektowano "bogatÄ…" strukturÄ™ danych jÄ™zyka C, skÅ‚adajÄ…cÄ… siÄ™ z losowych parametrÃ³w, w tym jednej tablicy znakÃ³w.

```c
struct simple_structure {
	char random_arr[3];
	int random_number;
	double random_double;
};
```

- nastÄ™pnie wykorzystano napisanÄ… strukturÄ™ oraz napisany wczeÅ›niej program do przesyÅ‚ania struktury *simple_structure* pomiÄ™dzy procesami przy uÅ¼yciu MPI_PACKED.

```c
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include "mpi.h"

struct simple_structure {
	char random_arr[3];
	int random_number;
	double random_double;
};

int main(int argc, char** argv) {

	struct simple_structure ss;
	int rank, ranksent, size, source, dest, tag, i, position, b_size, pack_size;
	double num;
	double a;
	void * buffer;
	struct simple_structure local;
	MPI_Status status;

	MPI_Init(&argc, &argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	
	MPI_Pack_size(1, MPI_INT, MPI_COMM_WORLD, &b_size);
	pack_size = b_size;
	MPI_Pack_size(1, MPI_CHAR, MPI_COMM_WORLD, &b_size);
	pack_size += b_size;
	MPI_Pack_size(3, MPI_DOUBLE, MPI_COMM_WORLD, &b_size);
	pack_size += b_size;
	
	buffer = (void*)malloc(3 * pack_size);
	
	position = 0;
	ss.random_number = 13;

	ss.random_double = 1.23456789;
	
	ss.random_arr[0] = 'x';
	ss.random_arr[1] = 'y';
	ss.random_arr[2] = 'z';

	if (rank == 0) {
		dest = 1;
		tag = 0;

		MPI_Pack(&ss.random_number, 1, MPI_INT, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Pack(&ss.random_double, 1, MPI_DOUBLE, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Pack(&ss.random_arr, 3, MPI_CHAR, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Send(buffer, position, MPI_PACKED, dest, tag, MPI_COMM_WORLD); ;
		printf("[proces %d] wyslal [%d, %lf, [%s]] do [proces %d]\n", rank, ss.random_number, ss.random_double,
			ss.random_arr, dest);
	}
	else if (rank == size - 1) {
		dest = 0;
		tag = 0;
		position = 0;
		MPI_Recv(buffer, pack_size, MPI_PACKED, rank - 1, tag, MPI_COMM_WORLD, &status);
		MPI_Unpack(buffer, pack_size, &position, &local.random_number, 1, MPI_INT, MPI_COMM_WORLD);
		MPI_Unpack(buffer, pack_size, &position, &local.random_double, 1, MPI_DOUBLE, MPI_COMM_WORLD);
		MPI_Unpack(buffer, pack_size, &position, &(local.random_arr), 3, MPI_CHAR, MPI_COMM_WORLD);
		printf("[proces %d] odebral [%d, %lf, [%s]] od [proces %d]\n", rank, local.random_number, local.random_double, local.random_arr, status.MPI_SOURCE);
	}
	else {
		dest = rank + 1;
		tag = 0;
		
		MPI_Recv(buffer, pack_size, MPI_PACKED, rank - 1, 0, MPI_COMM_WORLD, &status);
		MPI_Unpack(buffer, pack_size, &position, &local.random_number, 1, MPI_INT, MPI_COMM_WORLD);
		MPI_Unpack(buffer, pack_size, &position, &local.random_double, 1, MPI_DOUBLE, MPI_COMM_WORLD);
		MPI_Unpack(buffer, pack_size, &position, &local.random_arr, 3, MPI_CHAR, MPI_COMM_WORLD);
		printf("[proces %d] odebral [%d, %lf, [%s]] od [proces %d]\n", rank, local.random_number, local.random_double, local.random_arr, status.MPI_SOURCE);

		position = 0;
		
		local.random_number = local.random_number + 10;
		local.random_double = local.random_double + 0.15;
		MPI_Pack(&local.random_number, 1, MPI_INT, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Pack(&local.random_double, 1, MPI_DOUBLE, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Pack(&local.random_arr, 3, MPI_CHAR, buffer, pack_size, &position, MPI_COMM_WORLD);
		MPI_Send(buffer, position, MPI_PACKED, dest, tag, MPI_COMM_WORLD);
		printf("[proces %d] wyslal [%d, %lf, [%s]] do [proces %d]\n", rank, local.random_number, local.random_double, local.random_arr, dest);
	}

	MPI_Finalize();

	return(0);
}
```

- Sprawdzenie - struktura nie ulegÅ‚a zmianie podczas przesyÅ‚ania (sztafeta)

![images/Untitled%2053.png](images/Untitled%2053.png)

- celowa zmiana (jak w zaÅ‚Ä…czonym kodzie), dodawanie +10 do intÃ³w i +0.15 do doubli.

![images/Untitled%2054.png](images/Untitled%2054.png)

### Wnioski:

- Poza instalacjÄ… potrzebujemy biblioteki "mpi.h" do pisania programÃ³w
- Specyfikacja MPI okreÅ›la model przesyÅ‚ania komunikatÃ³w w paradygmacie "send-receive"
- Podstawowe procedury Å›rodowiska przesyÅ‚ania MPI to:

```java
int MPI_Init( int *pargc, char ***pargv) 
int MPI_Comm_size(MPI_Comm comm, int *psize)
int MPI_Comm_rank(MPI_Comm comm, int *prank)
int MPI_Finalize(void)
```

- MPI gwarantuje
    - postÄ™p podczas przesyÅ‚ania - tj. gdy zainicjujemy parÄ™ send-receive to co najmniej jedna z nich zostaje ukoÅ„czona
    - zachowanie porzÄ…dku przyjmowania tj zachowuje kolejnoÅ›cÄ‡ wysÅ‚ania - dot. komunikatÃ³w z tego samego ÅºrÃ³dÅ‚a, ten sam identyfikator, ten sam komuikator
- MPI nie gwarantuje:
    - uczciwoÅ›ci przy odbieraniu komunikatÃ³w z rÃ³Å¼nych ÅºrÃ³deÅ‚.
- zdarza siÄ™, Å¼e w trakcie realizacji procedur przesyÅ‚ania moÅ¼e wystÄ…piÄ‡ bÅ‚Ä…d zwiÄ…zany z przekroczeniem dostÄ™pnych zasobÃ³w systemowych
- Procedury dwupunktowego przesyÅ‚ania komunikatÃ³w wykorzystane w programie:
    - BLOKUJÄ„CE - gdy operacje komunikacji nie zostanÄ… ukoÅ„czone, wiÄ™c nie moÅ¼na zapewniÄ‡ bezpieczeÅ„stwa korzystania z zmiennych buforowych(argumentÃ³w) to ta procedura nie przekazuje sterowania dalej

```java
int MPI_Send(void* buf, int count, MPI_Datatype dtype, int dest, int tag,  
MPI_Comm comm)
â¢ int MPI_Recv(void *buf, int count, MPI_Datatype dtype, int src, int tag, 
MPI_Comm comm, MPI_Status *stat)
```

- MPI_PACK pakuje wiadomoÅ›Ä‡ w bufforze wysyÅ‚ania do buforu do wysÅ‚ania.

```c
MPI_PACK(inbuf, incount, datatype, outbuf, outsize, position, comm)
```

# **Sprawozdanie z laboratorium 13.**

15.01.2020, godz. 8:00

Wersja 2 - poprawiona.

**Cele:**

- Doskonalenie podstaw programowania z przesyÅ‚aniem komunikatÃ³w MPI.

**Wykonanie:**

- Utworzenie katalogu roboczego lab_13 i podkatalogu MPI_pi
- Opracowanie programu obliczajÄ…cego liczbÄ™ pi z szeregu Leibniza:

![images/Untitled%2055.png](images/Untitled%2055.png)

- Proces o randze 0 pobiera informacje o liczbie sumowanych skÅ‚adnikÃ³w, podanÄ… jako parametr przy uruchomieniu programu, z klawiatury itp.

```c
  if(rank == root){
  	//z pliku obliczP_PI
    printf("Podaj maksymalnÄ… liczbÄ™ wyrazÃ³w do obliczenia przybliÅ¼enia PI\n");
    scanf("%d", &max_chunks);
  }
```

- Liczba obliczanych skÅ‚adnikÃ³w szeregu zostaje rÃ³wno rozdzielona miÄ™dzy procesy liczÄ…ce sumy czÄ™Å›ciowe w celu zrÃ³wnowaÅ¼enia obciÄ…Å¼enia
- RozwiÄ…zujÄ™ problem w przypadku niepodzielnoÅ›ci liczby skÅ‚adnikÃ³w przez liczbÄ™ procesÃ³w liczÄ…cych
    - na stornie przedmiotu znajduje siÄ™ plik z wersjÄ… sekwencyjnÄ… obliczenia liczby pi: oblicz_PI.c
    - napisanie kodu rÃ³wnolegÅ‚ego polegaÅ‚o na zrÃ³wnolegleniu pÄ™tli
        1. Napisano standardowy szkielet jak w MPI_simple.c
        2. W miejsce wymiany komunikatÃ³w z MPI_simple.c wpisano odpowiednie przesyÅ‚ane komunikaty do obliczenia PI.
        3. zrÃ³wnoleglenie pÄ™tli wykonano jak w pthreads:
            1. na podstawie swojej rangi proces ustala, ktÃ³re iteracje wykonaÄ‡  tj . my_start, my_end, my_stride; czyli moj_poczatek, moj_koniec, moj_skok.

            2. TreÅ›Ä‡ pojedynczej iteracji jest identyczna jak w wersji sekwencyjnej.(z pliku oblicz_PI)

        4. Proces o randze 0 wczytuje dane poczÄ…tkowe (liczbÄ™ iteracji) i uzyskuje ostateczny wynik
        5. Do kompilacji uÅ¼yto Makefile z MPI_simple.tgz ze zmienionymi danymi.

```c
#include <stdlib.h>
#include <stdio.h>
#include <math.h>

#include "mpi.h"

#define SCALAR double

#ifndef M_PI 
#define M_PI (3.14159265358979323846)
#endif

void main(int argc, char** argv){ // program obliczania przybliÅ¼enia PI za pomocÄ… wzoru Leibniza
                                  // PI = 4 * ( 1 - 1/3 + 1/5 - 1/7 + 1/9 - 1/11 + itd. )
  //z z poprzednich zajec
	int rank, size, source, tag, i, root = 0;
  int max_chunks=0;

  MPI_Status status;

  //mpi init
  MPI_Init(&argc, &argv);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  if(rank == root){
  	//z pliku obliczP_PI
    printf("Podaj maksymalnÄ… liczbÄ™ wyrazÃ³w do obliczenia przybliÅ¼enia PI\n");
    scanf("%d", &max_chunks);
  }
  //mpi broadcasts a message from the process with the rank root to all other processes
  MPI_Bcast(&max_chunks, 1, MPI_INT, root, MPI_COMM_WORLD);
  int num = ceil(max_chunks / size);

  int start = rank * num;
  int end = start + num - 1;

  if(rank == (size-1)){
    end = max_chunks;
  }

  double partial_suma=0.0;

  //z pliku oblicz_PI
  SCALAR suma_plus=0.0;
  SCALAR suma_minus=0.0;

  printf("Proces %d, start %d, end %d\n", rank, start, end);
  for(i=start; i<=end; i++){
    
    int j = 1 + 4*i;
    
    suma_plus += 1.0/j;
    suma_minus += 1.0/(j+2.0);
    
    //printf("PI obliczone: %20.15lf, aktualna poprawka: %20.15lf\n",
    //  	 4*(suma_plus-suma_minus), 4.0/j-4.0/(j+2.0));
    
  }
  partial_suma = suma_plus - suma_minus;
  printf("partial_suma %f\n", partial_suma);

  double suma=0.0;

  //redukcja
  MPI_Reduce(&partial_suma, &suma, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);

  if(rank == root){
    printf("PI obliczone: \t\t\t%20.15lf\n", 4*(suma_plus-suma_minus));
    printf("PI z biblioteki matematycznej: \t%20.15lf\n", M_PI);
  }
  
  MPI_Finalize();
}
```

Wyniki dla rÃ³Å¼nych liczb procesÃ³w:

![images/Untitled%2056.png](images/Untitled%2056.png)

![images/Untitled%2057.png](images/Untitled%2057.png)

![images/Untitled%2058.png](images/Untitled%2058.png)

![images/Untitled%2059.png](images/Untitled%2059.png)

- jak widaÄ‡ wyniki sÄ… coraz to bardziej dokÅ‚adne, dla 1000 mamy rÃ³Å¼nicÄ™ na poziomie okoÅ‚o 0.004, a dla 1000000 rÃ³Å¼nica jest dla czÄ™Å›ci dopiero po 5 miejscu po przecinku.
- nastÄ™pnie pobrano paczkÄ™ MPI_mat_vec_row.tgz, rozpakowano
- uruchomiono:

![images/Untitled%2060.png](images/Untitled%2060.png)

- nastÄ™pnie zmodyfikowano kod tak aby zamieniÄ‡ wymianÄ™ komunikatÃ³w z pomocÄ… MPI_Send i MPI_Recv na procedury komunikacji grupowej.
- Komunikacja dotyczy
    - rozsyÅ‚ania danych poczÄ…tkowych z procesu o randze 0 do innych procesÃ³w
    - odbierania wyniku przez proces o randze 0 do innych procesÃ³w

KomunikacjÄ™ za pomocÄ… MPI_Send i MPI_Recv jak poniÅ¼ej:

![images/Untitled%2061.png](images/Untitled%2061.png)

zamieniono na wersjÄ™ z uÅ¼yciem MPI_Scatter

![images/Untitled%2062.png](images/Untitled%2062.png)

- proces 0 rozsyÅ‚a komynikaty do wszytskich innych procesÃ³w
- nastÄ™pnie, tak jak w programie z MPI_Send i MPI_Recv uÅ¼ywam MPI_Allgather i MPI_Barrier by zebraÄ‡ dane i MPI_Barrier by dokonaÄ‡ blokady, nastÄ™pnie drukuje czasy potrzebne na obliczenia rÃ³wnolegÅ‚e. MPI_Allgather to wariant funkcji MPI_Gather

![images/Untitled%2063.png](images/Untitled%2063.png)

- NastÄ™pnie odebrano dane przez proces 0 za pomocÄ… *MPI_Gather,* przebudowano wiÄ™c kod polegajÄ…cy na uÅ¼ywaniu MPI_Send na uÅ¼ycie MPI_Gather dla komunikacji grupowej.

Poprzedni kod:

![images/Untitled%2064.png](images/Untitled%2064.png)

Nowy kod:

![images/Untitled%2065.png](images/Untitled%2065.png)

Uruchomienie programu:

![images/Untitled%2066.png](images/Untitled%2066.png)

- Czas wykonania w wersji sekwencyjnej nieznacznie siÄ™ wydÅ‚uÅ¼yÅ‚ (byÄ‡ moÅ¼e zaburzenie przez MPI zgodnie z sugestiÄ…).
- Czas wykonania w wersji rÃ³wnolegÅ‚ej z komunikacjÄ… grupowÄ… jest krÃ³tszy, lecz rÃ³Å¼nica nie jest duÅ¼a.
- UproÅ›ciÅ‚a siÄ™ jednak znacznie budowa samego algorytmu, wiÄ™c uÅ¼ycie operacji grupowych jest ciekawym odpowiednikiem komunikacji z MPI_Send i MPI_Recv i zdecydowanie jest warta wiÄ™kszej uwagi. IloÅ›Ä‡ napisanego kodu, znaczÄ…co zmalaÅ‚a, wiÄ™c staÅ‚ siÄ™ teÅ¼ on bardziej przejrzysty.

### Wnioski z Ä‡wiczenia:

- NauczyÅ‚em siÄ™, Å¼e operacje komunikacji grupowej to operacje, w ramach ktÃ³rych ten sam komunikat (zbiÃ³r komunikatÃ³w) przesyÅ‚any jest pomiÄ™dzy >2 procesami
- NaleÅ¼y mieÄ‡ na uwadze, Å¼e nie moÅ¼na jasno zdefiniowaÄ‡ ich wydajnoÅ›ci, zaleÅ¼y to od wielu czynnikÃ³w, w tym architektury systemu komputerowego i sposobu przesyÅ‚ania komunikÃ³w w sieci poÅ‚Ä…czeÅ„
- RozrÃ³Å¼niamy:
    - broadcasting: jeden do wszystkich LUB wszyscy do wszystkich
    - scattering: jeden do wszystkich
    - gatering: wszyscy do jednego
    - reduction: wszyscy do jednego LUB wszyscy do wszystkich
    - wymiana wszyscy do wszystkich
- MPI_Scatter jest bardzo podobne do MPI_Bcast. PodstawowÄ… rÃ³Å¼nicÄ… jest to, Å¼e MPI_Bcast wysyÅ‚a dokÅ‚adnie te same dane do wszystkich procesÃ³w, podczas gdy MPIScatter moÅ¼e wysyÅ‚aÄ‡ rÃ³Å¼ne fragmenty tablicy do rÃ³Å¼nych procesÃ³w. (one to many)
- MPI_Gather to przeciwnoÅ›Ä‡ MPI_Scatter: bierze elementu od wielu procesÃ³w i zbiera je do jednego procesu. Jest to przydatne czÄ™sto w sortowaniu lub wyszukiwaniu rÃ³wnolegÅ‚ym. (many to one)
- MPI_Allgather pozwala wysyÅ‚aÄ‡ wiele elementÃ³w do wielu procesÃ³w. MoÅ¼na powiedzieÄ‡, Å¼e MPI_Allgather jest bardzo podobne do MPI_Gather, ale jakby poprzedzone przez MPI_Bcast

Kopia programu:

```c
#include <stdlib.h>
#include <stdio.h>
#include <math.h>

#include "mpi.h"
#include "pomiar_czasu.h"

#define WYMIAR 6400
#define ROZMIAR 40960000

//#define WYMIAR 7936
//#define ROZMIAR 62980096
//#define WYMIAR 19200
//#define ROZMIAR 368640000

void mat_vec(double* a, double* x, double* y, int n, int nt);
int main(int argc, char** argv)
{
	static double x[WYMIAR], y[WYMIAR], z[WYMIAR];
	double *a;
	double t1;
	int n, nt, i, j;
	const int ione = 1;
	const double done = 1.0;
	const double dzero = 0.0;
	int rank, size, source, dest, tag = 0;
	int n_wier, n_wier_last;
	MPI_Status status;

	MPI_Init(&argc, &argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	n = WYMIAR;

	if (rank == 0) {

		a = (double *)malloc((ROZMIAR + 1) * sizeof(double));

		for (i = 0; i < ROZMIAR; i++) a[i] = 1.0*i;
		for (i = 0; i < WYMIAR; i++) x[i] = 1.0*(WYMIAR - i);

		//printf("Podaj liczbe watkow: "); scanf("%d",&nt);
		nt = 1;

		printf("poczatek (wykonanie sekwencyjne)\n");

		inicjuj_czas();
		t1 = MPI_Wtime();
		mat_vec(a, x, y, n, nt);
		t1 = MPI_Wtime() - t1;
		drukuj_czas();

		printf("\tczas wykonania (zaburzony przez MPI?): %lf, Gflop/s: %lf, GB/s> %lf\n",
			t1, 2.0e-9*ROZMIAR / t1, (1.0 + 1.0 / n)*8.0e-9*ROZMIAR / t1);
	}

	if (size > 1) {

		int iter;
		/************** || *******************/

		for (i = 0; i < WYMIAR; i++) z[i] = 0.0;
		MPI_Bcast(&nt, 1, MPI_INT, 0, MPI_COMM_WORLD);

		// podzial wierszowy
		n_wier = ceil(WYMIAR / size);
		n_wier_last = WYMIAR - n_wier * (size - 1);

		if (n_wier != n_wier_last) {

			printf("This version does not work with WYMIAR not a multiple of size!\n");
			MPI_Finalize();
			exit(0);
		}

		if (rank > 0) 
		{
			for (i = 0; i < WYMIAR; i++) x[i] = 0.0;
			source = 0;
			a = (double*)malloc(WYMIAR*n_wier * sizeof(double));
		}
		//MPI SCATTER - SENDS FROM ONE PROCESS TO ALL OTHER PROCESSES IN COMMUNICATOR
		MPI_Scatter(a, n_wier*WYMIAR, MPI_DOUBLE, a, n_wier*WYMIAR, MPI_DOUBLE, 0, MPI_COMM_WORLD);
		MPI_Scatter(x, n_wier, MPI_DOUBLE, &x[rank*n_wier], n_wier, MPI_DOUBLE, 0, MPI_COMM_WORLD);

		//SR.. down
		if (rank == 0) 
		{
			inicjuj_czas();
			t1 = MPI_Wtime();
		}

		//GATHERS DATA FROM ALL TASKS AND DISTRIBUTE THE COMBINED DATA TO ALL TASKS
		MPI_Allgather(&x[rank*n_wier], n_wier, MPI_DOUBLE, x, n_wier, MPI_DOUBLE, MPI_COMM_WORLD);

		for (i = 0; i < n_wier; i++) 
		{
			double t = 0.0;
			int ni = n * i;
			for (j = 0; j < n; j++) t += a[ni + j] * x[j];
			z[i] = t;
		}

		//BLOCKS UNTIL ALL PROCESSES IN THE COMMUNICATOR HAVE REACHED THIS ROUTINE.
		MPI_Barrier(MPI_COMM_WORLD);

		if (rank == 0) 
		{
			t1 = MPI_Wtime() - t1;
			printf("Werja rownolegla MPI\n");
			drukuj_czas();
			printf("\tczas wykonania: %lf, Gflop/s: %lf, GB/s> %lf\n",
			t1, 2.0e-9*ROZMIAR / t1, (1.0 + 1.0 / n)*8.0e-9*ROZMIAR / t1);
		}
		//SR.. up
		
		//GATHERS TOGETHER VALUES FROM A GROUO OF PROCESSES
		MPI_Gather(z, n_wier, MPI_DOUBLE, &z[(rank)*n_wier], n_wier, MPI_DOUBLE, 0, MPI_COMM_WORLD);

		if (rank == 0)
		{
			for (i = 0; i < WYMIAR; i++) 
			{
				if (fabs(y[i] - z[i]) > 1.e-9*z[i]) printf("Blad! i=%d, y[i]=%lf, z[i]=%lf\n", i, y[i], z[i]);
			}
		}
	}
	
	MPI_Finalize();

	return(0);

}
```

# **Sprawozdanie z laboratorium 15 (14)**

22.01.2020, godz. 8:00

Spis treÅ›ci:

**Cele:**

- Doskonalenie umiejÄ™tnoÅ›ci analizy wydajnoÅ›ci programÃ³w rÃ³wnolegÅ‚ych.

**Wykonanie:**

- Utworzono katalog roboczy lab_15 i podkatalog folder_calka.
- Skopiowano ze strony przedmiotu PR MPI_calka.tgz - program liczÄ…cy wartoÅ›Ä‡ caÅ‚ki, skompilowano i dokonano obliczeÅ„.
- UÅ¼yto odpowiednio skonfigurowanego Makefile
- Dokonano pomiarÃ³w dla liczby procesÃ³w liczÄ…cych 1,2,4,8.
- Obliczenia przeprowadzono dla wersji do debugowania i wersji zoptymalizowanej do obliczeÅ„
- Wyniki przedstawiono na wykresach.

## Zadanie 1. calka.c

![images/Untitled%2067.png](images/Untitled%2067.png)

Dla 1 procesu:

- **Dla 1: wersja zoptymalizowana do liczenia czasu:**

![images/Untitled%2068.png](images/Untitled%2068.png)

- **Wersja niezoptymalizowana:**

![images/Untitled%2069.png](images/Untitled%2069.png)

### Dla 2 procesÃ³w:

- Dla 2: wersja zoptymalizowana:

![images/Untitled%2070.png](images/Untitled%2070.png)

- 2: wersja niezoptymalizowana:

![images/Untitled%2071.png](images/Untitled%2071.png)

### Dla 4 procesÃ³w:

- Dla 4: wersja zoptymalizowana:

![images/Untitled%2072.png](images/Untitled%2072.png)

- Dla 4: wersja niezoptymalizowana:

![images/Untitled%2073.png](images/Untitled%2073.png)

### Dla 8 procesÃ³w:

- Dla 8: wersja zoptymalizowana:

![images/Untitled%2074.png](images/Untitled%2074.png)

- Dla 8: wersja niezoptymalizowana:

![images/Untitled%2075.png](images/Untitled%2075.png)

### Analiza wynikÃ³w:

![images/Untitled%2076.png](images/Untitled%2076.png)

Do obliczenia przyspieszenia i efektywnoÅ›ci posÅ‚uÅ¼ono siÄ™ nastÄ™pujÄ…cymi wzorami

![images/Untitled%2077.png](images/Untitled%2077.png)

![images/Untitled%2078.png](images/Untitled%2078.png)

### Wykresy dla wersji zoptymalizowanej do obliczeÅ„:

- do pomiarÃ³w wziÄ™to minimalny czas wykonania, po >7 prÃ³bach.
- odsuniÄ™te znacznie raÅ¼Ä…ce wyniki

    ![images/Untitled%2079.png](images/Untitled%2079.png)

    - jak widaÄ‡ czas, zgodnie z oczekiwaniami, spada wraz z zwiÄ™kszaniem liczby procesÃ³w biorÄ…cych udziaÅ‚ w obliczeniach

    ![images/Untitled%2080.png](images/Untitled%2080.png)

    - Przyspieszenie obliczeÅ„ roÅ›nie, wraz z udziaÅ‚em wiÄ™kszej liczby procesÃ³w.

    ![images/Untitled%2081.png](images/Untitled%2081.png)

    - EfektywnoÅ›Ä‡ zrÃ³wnoleglenia jednak spada. Wraz ze wzrostem liczby uÅ¼ywanych procesÃ³w za efektywnoÅ›Ä‡ optymalnÄ… moÅ¼na by byÅ‚o przyjÄ…Ä‡ mnoÅ¼nik zwiÄ™kszenia rÃ³wny liczbie procesÃ³w biorÄ…cych udziaÅ‚, tutaj pomimo nawet 3,58 razowego wzrostu przyÅ›pieszenia obliczeÅ„ mamy do czynienia z efektywnoÅ›ciÄ… na poziomie 0.447 poniewaÅ¼ za optymalny wzrost przyÅ›pieszenia moÅ¼naby byÅ‚o by uznaÄ‡ 8, poniewaÅ¼ tyle procesÃ³w braÅ‚o udziaÅ‚ w obliczeniach zamiast jednego

### PorÃ³wnanie wersji zoptymalizowanej i niezoptymalizowanej:

![images/Untitled%2082.png](images/Untitled%2082.png)

![images/Untitled%2083.png](images/Untitled%2083.png)

- przyÅ›pieszenie obliczeÅ„ jest juÅ¼ zauwaÅ¼alne juÅ¼ od obliczeÅ„ dla jednego procesu, a wraz z idÄ…cym wzrostem procesÃ³w biorÄ…cych udziaÅ‚ w obliczeniach moÅ¼na uznaÄ‡, Å¼e rÃ³Å¼nica miÄ™dzy wersjÄ… zoptymalizowanÄ… a niezoptymalizowanÄ… jest doÅ›Ä‡ maÅ‚a, roÅ›nie, ale utrzymuje staÅ‚Ä… rÃ³Å¼nicÄ™ biorÄ…c pod uwagÄ™ ogÃ³lny wzrost wartoÅ›ci
- wiÄ™ksze przyÅ›pieszenie jest cechÄ… wersji niezoptymalizowanej - tutaj widaÄ‡, jak z poczÄ…tkowej rÃ³Å¼nicy (0.72 i 0.86 odpowiednio wer. zopt. i niezopt.) dochodzimy do ok. 0.20 dla obu wersji gdy przeprowadzamy obliczenia na 8 procesach.
- efektywnoÅ›Ä‡ zrÃ³wnoleglenia jest funkcjÄ… przyÅ›pieszenia, wiÄ™c rÃ³wnieÅ¼ obserwujemy wiÄ™kszÄ… efektywnoÅ›Ä‡ w zrÃ³wnoleglaniu wersji niezoptymalizowanej

### Przeprowadzenie testÃ³w skalowalnoÅ›ci

- po zwiÄ™kszeniu liczby N*2 i 2 procesÃ³w

![images/Untitled%2084.png](images/Untitled%2084.png)

- Dla N*4 i 4 procesÃ³w:

![images/Untitled%2085.png](images/Untitled%2085.png)

- Dla N*8 i 8 procesÃ³w:

![images/Untitled%2086.png](images/Untitled%2086.png)

Dane minimalne: ( po selekcji - wybÃ³r min.):

![images/Untitled%2087.png](images/Untitled%2087.png)

Wykres:

![images/Untitled%2088.png](images/Untitled%2088.png)

- jak widaÄ‡, gdy przeprowadzamy test skalowalnoÅ›ci w sensie sÅ‚abym, czyli o staÅ‚ym rozmiarze zadania dla kaÅ¼dego wÄ…tku - jest to liczba operacji arytmetycznych czyli liczba iteracji w obl. caÅ‚ki to czas wykonania roÅ›nie

## Zadanie 2: mat_vec_row_omp

- uÅ¼yto programu do obliczania iloczynu macierz-wektor w Å›rodowisku OpenMP, rozpakowanego ze strony przedmiotu

### Wydruki:

NUM THREADS 1

![images/Untitled%2089.png](images/Untitled%2089.png)

NUM THREADS 2

![images/Untitled%2090.png](images/Untitled%2090.png)

NUM THREADS 4

![images/Untitled%2091.png](images/Untitled%2091.png)

NUM THREADS 8 

![images/Untitled%2092.png](images/Untitled%2092.png)

Wersja do debugowania:

```jsx
OPT = -g -DDEBUG -p 

```

Wersja do optymalizacji:

```jsx
OPT = -O3 -fopenmp -m64
```

### Wyniki:

![images/Untitled%2093.png](images/Untitled%2093.png)

### Wykresy:

![images/Untitled%2094.png](images/Untitled%2094.png)

![images/Untitled%2095.png](images/Untitled%2095.png)

![images/Untitled%2096.png](images/Untitled%2096.png)

- przeprowadzone obliczenia dla drugiego przykÅ‚ady cechujÄ… podobne zaleÅ¼noÅ›ci jak opisane dla przykÅ‚adu pierwszego, co potwierdza ich poprawnoÅ›Ä‡

![images/Untitled%2097.png](images/Untitled%2097.png)

![images/Untitled%2098.png](images/Untitled%2098.png)

- jeÅ›li chodzi o porÃ³wnanie wersji zoptymalizowanej i niezoptymalizowanej dla drugiego przykÅ‚adu obserwujemy bardzo zbliÅ¼one wyniki. AnalizujÄ…c szczegÃ³Å‚owo, dla wersji zoptymalizowanej efektywnoÅ›Ä‡ tej optymalizacji jest zauwaÅ¼alnie wiÄ™ksza juÅ¼ dla 8 procesÃ³w, dla 4 i 2 procesÃ³w jest minimalnie wiÄ™ksza.

### StaÅ‚y rozmiar pliku.

- nastÄ™pnie zmieniono WYMIAR czyli liczbÄ™ operacji tak aby liczba wÄ…tkÃ³w byÅ‚a proporcjonalna do liczby operacji

przykÅ‚adowe wywoÅ‚anie: 4 procesy

![images/Untitled%2099.png](images/Untitled%2099.png)

![images/Untitled%20100.png](images/Untitled%20100.png)

![images/Untitled%20101.png](images/Untitled%20101.png)

przykÅ‚adowe wywoÅ‚anie 8 procesÃ³w:

![images/Untitled%20102.png](images/Untitled%20102.png)

![images/Untitled%20103.png](images/Untitled%20103.png)

![images/Untitled%20104.png](images/Untitled%20104.png)

![images/Untitled%20105.png](images/Untitled%20105.png)

- Podczas wykonania dla staÅ‚ego rozmiaru zadania obserwujemy wzrost czasÃ³w aÅ¼ do 4 procesÃ³w, a dla 8 procesÃ³w czas wykonania maleje.
- Jest odstÄ™pstwo od poprzedniego przykÅ‚adu, gdzie wzrost charakteryzowaÅ‚ wiÄ™kszy czas dla coraz to wiÄ™kszej liczby procesÃ³w.

## Wnioski:

- poniewaÅ¼ wiÄ™kszÄ… efektywnoÅ›Ä‡ zrÃ³wnoleglania obserwujemy dla wersji nieoptymalnej niÅ¼ dla wersji optymalnej, moÅ¼na zauwaÅ¼yÄ‡, Å¼e dla tej wersji opÅ‚aca siÄ™ szczegÃ³lnie przeprowadzaÄ‡ obliczenia rÃ³wnolegÅ‚e
- pomimo uÅ¼ycia wielu procesÃ³w, wzrost szybkoÅ›ci obliczeÅ„ nigdy nie bÄ™dzie liniowy do tej liczby, zawsze szybkoÅ›Ä‡ bÄ™dzie rosÅ‚a, ale nigdy tak jak moglibyÅ›my siÄ™ spodziewaÄ‡ w idealnym Å›wiecie.
- Podczas przeprowadzenia testu skalowalnoÅ›ci dowiedziono, Å¼e gdy zwiÄ™kszamy liczbÄ™ procesÃ³w, ale nakÅ‚adamy na kaÅ¼dy proces taki sam rozmiar zadania to czas wykonania roÅ›nie i jest znacznie wiÄ™kszy niÅ¼ przy obliczeniach na jednym procesie oraz szczegÃ³lnie na wielu procesach, gdy mamy rozmiar zadania podzielony na te procesy. W drugim zadaniu znaleziono odstÄ™pstwo od tej reguÅ‚y wykonujÄ…c obliczenia dla 8 procesÃ³w w czasie mniejszym niÅ¼ dla 4 procesÃ³w co udokumentowano screenshotami. Pomimo, Å¼e starano siÄ™ wykonywaÄ‡ obliczenia na serwerze Estera w rÃ³Å¼nych odstÄ™pach czasowych, wyniki siÄ™ utrzymywaÅ‚y na tym samym poziomie.